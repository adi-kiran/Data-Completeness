{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preloading all schemas and categories\n",
    "all_schemas={}\n",
    "all_categories={}\n",
    "with open(\"final_schema.txt\") as ip_file:\n",
    "    for line in ip_file.readlines():\n",
    "        json_obj=json.loads(line)\n",
    "        all_schemas[json_obj[\"filename\"]]=json_obj[\"schema\"]\n",
    "        all_categories[json_obj[\"filename\"]]=json_obj[\"categories\"]\n",
    "        \n",
    "#preloading the candidate keys\n",
    "with open(\"Candidate_key_dict.txt\",'r') as ip_file:\n",
    "    cand_key=json.load(ip_file)\n",
    "\n",
    "#preloading column and category similarity values of tables\n",
    "with open(\"cos_similarity.txt\",'r') as ip_file:\n",
    "    json_object=json.load(ip_file)\n",
    "col_sim = json_object[\"column_similarity\"]\n",
    "cat_sim = json_object[\"category_similarity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all functions needed to generate ontologies\n",
    "def get_synonyms(word):\n",
    "    meanings=set()\n",
    "    for synset in wordnet.synsets(word,pos=wordnet.NOUN):\n",
    "        for lemma in synset.lemmas():\n",
    "            meanings.add(lemma.name())\n",
    "    for synset in wordnet.synsets(word,pos=wordnet.NOUN):\n",
    "        for hypernym in synset.hypernyms():\n",
    "            meanings.add(hypernym.lemma_names()[0])\n",
    "    meanings.add(word)\n",
    "    return list(meanings)\n",
    "\n",
    "def generate_list_ontology(list1):\n",
    "    ontology={}\n",
    "    for word in list1:\n",
    "        ontology[word]=get_synonyms(word)\n",
    "    return ontology\n",
    "\n",
    "def generate_schema_ontology(input_schema):\n",
    "    ontology={}\n",
    "    for col in input_schema:\n",
    "        ontology[col]=get_synonyms(col)\n",
    "    return ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate cos similarity between two lists\n",
    "def cos_sim(list1, list2):\n",
    "    terms = set(list1).union(list2)\n",
    "    intersect = set(list1) & set(list2)\n",
    "    others = (set(list1)-intersect).union(set(list2)-intersect)\n",
    "    product=0\n",
    "    for word in terms:\n",
    "        if word in intersect:\n",
    "            product+=1\n",
    "    l1mag = math.sqrt(len(list1))\n",
    "    l2mag = math.sqrt(len(list2))\n",
    "    if len(list1)==0 or len(list2)==0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return product / (l1mag * l2mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if input has only schema(columns and their dataypes)\n",
    "def col_only_list(input_schema,input_sch_onto):\n",
    "    possible_tables={}\n",
    "    for file in all_schemas:\n",
    "        schema=all_schemas[file]\n",
    "        for col,d_type in schema.items():\n",
    "            if (col in input_schema) and (input_schema[col]==d_type):\n",
    "                if file in possible_tables:\n",
    "                    possible_tables[file].append(col)\n",
    "                else:\n",
    "                    possible_tables[file]=list(col)\n",
    "            else:\n",
    "                for a in input_sch_onto:\n",
    "                    if (col in input_sch_onto[a]) and (input_schema[a]==d_type):\n",
    "                        if file in possible_tables:\n",
    "                            possible_tables[file].append(a)\n",
    "                        else:\n",
    "                            possible_tables[file]=list(a)\n",
    "    return possible_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if input has categories as well as schema(columns and their dataypes)\n",
    "# we consider it a match under the assumption that at least 75% category match exists\n",
    "def cat_and_col_list(input_categories,input_cat_onto,input_schema,input_sch_onto):\n",
    "    possible_tables={}\n",
    "    for file in all_categories:\n",
    "        cat_list=[]\n",
    "        category=all_categories[file]\n",
    "        for cat in category:\n",
    "            if cat in input_categories:\n",
    "                cat_list.append(cat)\n",
    "            else:\n",
    "                for cat1 in input_cat_onto:\n",
    "                    if cat in input_cat_onto[cat1]:\n",
    "                        cat_list.append(cat1)\n",
    "        cos_val=cos_sim(cat_list,input_categories)\n",
    "        if cos_val > 0.75 :\n",
    "            schema=all_schemas[file]\n",
    "            for col,d_type in schema.items():\n",
    "                if (col in input_schema) and (input_schema[col]==d_type):\n",
    "                    if file in possible_tables:\n",
    "                        possible_tables[file].append(col)\n",
    "                    else:\n",
    "                        possible_tables[file]=list(col)\n",
    "                else:\n",
    "                    for a in input_sch_onto:\n",
    "                        if (col in input_sch_onto[a]) and (input_schema[a]==d_type):\n",
    "                            if file in possible_tables:\n",
    "                                possible_tables[file].append(a)\n",
    "                            else:\n",
    "                                possible_tables[file]=list(a)\n",
    "    return possible_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the given input file names , find the columns that match if any.\n",
    "#If no colmns match , then don't do anything.\n",
    "#If columns match , then check if the columns are present in the list of candidate keys.\n",
    "#If not present in list of candidate keys , just check data.\n",
    "#If present in list of candidate keys and data of those columns match , append the others columns to any of the table to extend the table schema.\n",
    "#If present in list of candidate keys but data dont match , then increse volume by adding the rows to one of the table , but will create a lot of NULL values. Undesirable.\n",
    "#If all the columns match , then to each table just add another column with the table name and append the two tables.\n",
    "\n",
    "def merge(fname1,fname2):\n",
    "    matching_columns = {}  #resultant list containing the matching columns.\n",
    "    #getting schema for tables.\n",
    "    f1 = all_schemas[fname1]\n",
    "    f2 = all_schemas[fname2]\n",
    "    #Retrieving the column names and generating the ontologies for one of the tables columns\n",
    "    f1_cols = list(f1)\n",
    "    f1_onto = generate_list_ontology(f1_cols)\n",
    "    #finding columns that match.     \n",
    "    for col , d_type in f2.items():\n",
    "        if(col in f1_cols):\n",
    "            matching_columns[col] = col\n",
    "        else:\n",
    "            for col_t1 in f1_onto:\n",
    "                if(col in f1_onto[col_t1]):\n",
    "                    matching_columns[col_t1] = col\n",
    "                    break\n",
    "    #Now the varaible matching_columns contains a list of names of columns that match between the two tables.\n",
    "    print(matching_columns)\n",
    "    t1 = cand_key[fname1]\n",
    "    t2 = cand_key[fname2]\n",
    "    mat_cols={}\n",
    "    for key , value in matching_columns.items():\n",
    "        if(key in t1 or value in t2):\n",
    "            mat_cols[key]=value\n",
    "    merge_tables(fname1,fname2,mat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tables(fname1,fname2,cols):\n",
    "    t1 = pd.read_csv(fname1)\n",
    "    t2 = pd.read_csv(fname2)\n",
    "    l=len(cols)\n",
    "    if l!=0:\n",
    "        for name1,name2 in cols.items():\n",
    "            t2.rename(columns = {name2:name1},inplace=True)\n",
    "    t3=t1.merge(t2,how='outer')\n",
    "    print(fname1+' and '+fname2+' gives : ')\n",
    "    display(t3)\n",
    "    return t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_possible_matches():\n",
    "    with open(\"input.txt\",'r') as ip_file:\n",
    "        json_object=json.load(ip_file)\n",
    "    input_schema=json_object[\"schema\"]\n",
    "    input_sch_onto=generate_schema_ontology(input_schema)\n",
    "    if \"categories\" in json_object:\n",
    "        print('category and schema')\n",
    "        input_categories=json_object[\"categories\"]\n",
    "        input_cat_onto=generate_list_ontology(input_categories)\n",
    "        possible_tables=cat_and_col_list(input_categories,input_cat_onto,input_schema,input_sch_onto)\n",
    "    else:\n",
    "        print('only schema')\n",
    "        possible_tables=col_only_list(input_schema,input_sch_onto)\n",
    "    matching_tables=[]\n",
    "    for i in possible_tables:\n",
    "        cos_val=cos_sim(possible_tables[i],list(input_schema))\n",
    "        if cos_val>0.50:\n",
    "            matching_tables.append(i)\n",
    "    return matching_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches():\n",
    "    matching_tables=check_possible_matches()\n",
    "    op_str1='output_folder/'\n",
    "    op_str2='.txt'\n",
    "    f=open(\"matches.txt\",'w')\n",
    "    x=len(matching_tables)\n",
    "    print(matching_tables)\n",
    "    if x==0:\n",
    "        print(\"NO MATCHES FOUND\",file=f)\n",
    "    elif x==1:\n",
    "        print(matching_tables[0],file=f)\n",
    "    else:\n",
    "        ctr=0\n",
    "        for i in range(x-1):\n",
    "            for j in range(i+1,x):\n",
    "                a=matching_tables[i]+' : '+matching_tables[j]\n",
    "                b=matching_tables[j]+' : '+matching_tables[i]\n",
    "                if (a in cat_sim):\n",
    "                    if cat_sim[a]>=.1 and col_sim[a]>=0.1:\n",
    "                        res=merge(matching_tables[i],matching_tables[j])\n",
    "                        display(res)\n",
    "                        ctr+=1\n",
    "                        op_string=op_str1+str(ctr)+op_str2\n",
    "                        print('hello')\n",
    "                        res.to_csv(op_string,sep=',', index=False)\n",
    "                        print(matching_tables[i]+' and '+matching_tables[j],file=f)\n",
    "                        print(matching_tables[i]+' and '+matching_tables[j])\n",
    "                elif (b in cat_sim):\n",
    "                    if cat_sim[b]>=.1 and col_sim[b]>=0.1:\n",
    "                        res=merge(matching_tables[i],matching_tables[j])\n",
    "                        ctr+=1\n",
    "                        op_string=op_str1+str(ctr)+op_str2\n",
    "                        res.to_csv(op_string,sep=',', index=False)\n",
    "                        print(matching_tables[i]+' and '+matching_tables[j],file=f)\n",
    "                        print(matching_tables[i]+' and '+matching_tables[j])\n",
    "                else:\n",
    "                    pass\n",
    "        for i in matching_tables:\n",
    "            print(i,file=f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only schema\n",
      "['202_117.csv', '204_100.csv']\n",
      "{'Date': 'Date', 'Nationality': 'Nationality', 'Tonnage': 'Tonnage', 'Fate': 'Fate'}\n",
      "202_117.csv and 204_100.csv gives : \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ship</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>Tonnage</th>\n",
       "      <th>Fate</th>\n",
       "      <th>Name of ship</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19 August 1941</td>\n",
       "      <td>SS Aguila</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>3255</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27 November 1941</td>\n",
       "      <td>HMAS Parramatta</td>\n",
       "      <td>Royal Australian Navy</td>\n",
       "      <td>1060</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23 December 1941</td>\n",
       "      <td>SS Shuntien</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>3059</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26 December 1941</td>\n",
       "      <td>SS Warszawa</td>\n",
       "      <td>Poland</td>\n",
       "      <td>2487</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 June 1942</td>\n",
       "      <td>MV Athene</td>\n",
       "      <td>Norway</td>\n",
       "      <td>4681</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10 June 1942</td>\n",
       "      <td>SS Brambleleaf</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>5917</td>\n",
       "      <td>Damaged</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5 June 1940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>798</td>\n",
       "      <td>Sunk at 58°48′N 08°45′W﻿ / ﻿58.800°N 8.750°W</td>\n",
       "      <td>SS Stancor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7 June 1940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>4212</td>\n",
       "      <td>Sunk at 55°33′N 08°26′W﻿ / ﻿55.550°N 8.433°W</td>\n",
       "      <td>SS Frances Massey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7 June 1940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>5888</td>\n",
       "      <td>Damaged at 55°33′N 08°26′W﻿ / ﻿55.550°N 8.433°W</td>\n",
       "      <td>SS Eros</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11 June 1940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Greece</td>\n",
       "      <td>2375</td>\n",
       "      <td>Sunk at 44°04′N 12°30′W﻿ / ﻿44.067°N 12.500°W</td>\n",
       "      <td>SS Violando N Goulandris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>19 June 1940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Norway</td>\n",
       "      <td>6607</td>\n",
       "      <td>Sunk at 45°10′N 11°50′W﻿ / ﻿45.167°N 11.833°W</td>\n",
       "      <td>MV Tudor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>19 June 1940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>3164</td>\n",
       "      <td>Sunk at 45°00′N 11°21′W﻿ / ﻿45.000°N 11.350°W</td>\n",
       "      <td>SS Baron Loudoun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>19 June 1940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>5661</td>\n",
       "      <td>Sunk at 45°00′N 11°21′W﻿ / ﻿45.000°N 11.350°W</td>\n",
       "      <td>SS British Monarch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>20 June 1940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>7493</td>\n",
       "      <td>Sunk at 43°34′N 14°20′W﻿ / ﻿43.567°N 14.333°W</td>\n",
       "      <td>MV Moerdrecht</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16 August 1940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sweden</td>\n",
       "      <td>2325</td>\n",
       "      <td>Sunk at 57°10′N 16°37′W﻿ / ﻿57.167°N 16.617°W</td>\n",
       "      <td>SS Hedrun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>19 August 1940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>7590</td>\n",
       "      <td>Sunk at 55°28′N 15°10′W﻿ / ﻿55.467°N 15.167°W</td>\n",
       "      <td>SS Ville de Gand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24 August 1940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>6666</td>\n",
       "      <td>Sunk at 57°24′N 11°21′W﻿ / ﻿57.400°N 11.350°W</td>\n",
       "      <td>SS La Brea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25 August 1940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>5763</td>\n",
       "      <td>Sunk at 58°30′N 10°15′W﻿ / ﻿58.500°N 10.250°W</td>\n",
       "      <td>SS Empire Merlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>25 August 1940</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>6825</td>\n",
       "      <td>Sunk at 58°24′N 11°25′W﻿ / ﻿58.400°N 11.417°W</td>\n",
       "      <td>MV Athelcrest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Date             Ship            Nationality  Tonnage  \\\n",
       "0     19 August 1941        SS Aguila         United Kingdom     3255   \n",
       "1   27 November 1941  HMAS Parramatta  Royal Australian Navy     1060   \n",
       "2   23 December 1941      SS Shuntien         United Kingdom     3059   \n",
       "3   26 December 1941      SS Warszawa                 Poland     2487   \n",
       "4       10 June 1942        MV Athene                 Norway     4681   \n",
       "5       10 June 1942   SS Brambleleaf         United Kingdom     5917   \n",
       "6        5 June 1940              NaN         United Kingdom      798   \n",
       "7        7 June 1940              NaN         United Kingdom     4212   \n",
       "8        7 June 1940              NaN         United Kingdom     5888   \n",
       "9       11 June 1940              NaN                 Greece     2375   \n",
       "10      19 June 1940              NaN                 Norway     6607   \n",
       "11      19 June 1940              NaN         United Kingdom     3164   \n",
       "12      19 June 1940              NaN         United Kingdom     5661   \n",
       "13      20 June 1940              NaN            Netherlands     7493   \n",
       "14    16 August 1940              NaN                 Sweden     2325   \n",
       "15    19 August 1940              NaN                Belgium     7590   \n",
       "16    24 August 1940              NaN         United Kingdom     6666   \n",
       "17    25 August 1940              NaN         United Kingdom     5763   \n",
       "18    25 August 1940              NaN         United Kingdom     6825   \n",
       "\n",
       "                                               Fate              Name of ship  \n",
       "0                                              Sunk                       NaN  \n",
       "1                                              Sunk                       NaN  \n",
       "2                                              Sunk                       NaN  \n",
       "3                                              Sunk                       NaN  \n",
       "4                                              Sunk                       NaN  \n",
       "5                                           Damaged                       NaN  \n",
       "6      Sunk at 58°48′N 08°45′W﻿ / ﻿58.800°N 8.750°W                SS Stancor  \n",
       "7      Sunk at 55°33′N 08°26′W﻿ / ﻿55.550°N 8.433°W         SS Frances Massey  \n",
       "8   Damaged at 55°33′N 08°26′W﻿ / ﻿55.550°N 8.433°W                   SS Eros  \n",
       "9     Sunk at 44°04′N 12°30′W﻿ / ﻿44.067°N 12.500°W  SS Violando N Goulandris  \n",
       "10    Sunk at 45°10′N 11°50′W﻿ / ﻿45.167°N 11.833°W                  MV Tudor  \n",
       "11    Sunk at 45°00′N 11°21′W﻿ / ﻿45.000°N 11.350°W          SS Baron Loudoun  \n",
       "12    Sunk at 45°00′N 11°21′W﻿ / ﻿45.000°N 11.350°W        SS British Monarch  \n",
       "13    Sunk at 43°34′N 14°20′W﻿ / ﻿43.567°N 14.333°W             MV Moerdrecht  \n",
       "14    Sunk at 57°10′N 16°37′W﻿ / ﻿57.167°N 16.617°W                 SS Hedrun  \n",
       "15    Sunk at 55°28′N 15°10′W﻿ / ﻿55.467°N 15.167°W          SS Ville de Gand  \n",
       "16    Sunk at 57°24′N 11°21′W﻿ / ﻿57.400°N 11.350°W                SS La Brea  \n",
       "17    Sunk at 58°30′N 10°15′W﻿ / ﻿58.500°N 10.250°W          SS Empire Merlin  \n",
       "18    Sunk at 58°24′N 11°25′W﻿ / ﻿58.400°N 11.417°W             MV Athelcrest  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-4993ff85b159>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_matches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-77-c5e1b7cce2b2>\u001b[0m in \u001b[0;36mget_matches\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m                         \u001b[0mop_string\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mop_str1\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mop_str2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hello'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                         \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_string\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatching_tables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' and '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmatching_tables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatching_tables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m' and '\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmatching_tables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "get_matches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions=[\"sum\",\"product\",\"power\",\"square root\",\"quotient\",\"mod\",\"subtotal\",\"ceiling\",\"floor\",\"int\",\"trunc\",\"count\",\"countif\",\"counta\",\"max\",\"min\",\"average\",\"averagea\",\"median\",\"mode\",\"stdev\",\"var\",\"frequency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "x=2\n",
    "for i in range(x-1):\n",
    "    for j in range(i+1,x):\n",
    "        print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
