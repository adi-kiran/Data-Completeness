{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preloading all schemas and categories\n",
    "all_schemas={}\n",
    "all_categories={}\n",
    "with open(\"final_schema.txt\") as ip_file:\n",
    "    for line in ip_file.readlines():\n",
    "        json_obj=json.loads(line)\n",
    "        all_schemas[json_obj[\"filename\"]]=json_obj[\"schema\"]\n",
    "        all_categories[json_obj[\"filename\"]]=json_obj[\"categories\"]\n",
    "        \n",
    "#preloading the candidate keys\n",
    "with open(\"Candidate_key_dict.txt\",'r') as ip_file:\n",
    "    cand_key=json.load(ip_file)\n",
    "\n",
    "#preloading column and category similarity values of tables\n",
    "with open(\"cos_similarity.txt\",'r') as ip_file:\n",
    "    json_object=json.load(ip_file)\n",
    "col_sim = json_object[\"column_similarity\"]\n",
    "cat_sim = json_object[\"category_similarity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all functions needed to generate ontologies\n",
    "def get_synonyms(word):\n",
    "    meanings=set()\n",
    "    for synset in wordnet.synsets(word,pos=wordnet.NOUN):\n",
    "        for lemma in synset.lemmas():\n",
    "            meanings.add(lemma.name())\n",
    "    for synset in wordnet.synsets(word,pos=wordnet.NOUN):\n",
    "        for hypernym in synset.hypernyms():\n",
    "            meanings.add(hypernym.lemma_names()[0])\n",
    "    meanings.add(word)\n",
    "    return list(meanings)\n",
    "\n",
    "def generate_list_ontology(list1):\n",
    "    ontology={}\n",
    "    for word in list1:\n",
    "        ontology[word]=get_synonyms(word)\n",
    "    return ontology\n",
    "\n",
    "def generate_schema_ontology(input_schema):\n",
    "    ontology={}\n",
    "    for col in input_schema:\n",
    "        ontology[col]=get_synonyms(col)\n",
    "    return ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate cos similarity between two lists\n",
    "def cos_sim(list1, list2):\n",
    "    terms = set(list1).union(list2)\n",
    "    intersect = set(list1) & set(list2)\n",
    "    others = (set(list1)-intersect).union(set(list2)-intersect)\n",
    "    product=0\n",
    "    for word in terms:\n",
    "        if word in intersect:\n",
    "            product+=1\n",
    "    l1mag = math.sqrt(len(list1))\n",
    "    l2mag = math.sqrt(len(list2))\n",
    "    if len(list1)==0 or len(list2)==0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return product / (l1mag * l2mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if input has only schema(columns and their dataypes)\n",
    "def col_only_list(input_schema,input_sch_onto):\n",
    "    possible_tables={}\n",
    "    for file in all_schemas:\n",
    "        schema=all_schemas[file]\n",
    "        for col,d_type in schema.items():\n",
    "            if (col in input_schema) and (input_schema[col]==d_type):\n",
    "                if file in possible_tables:\n",
    "                    possible_tables[file].append(col)\n",
    "                else:\n",
    "                    possible_tables[file]=[]\n",
    "                    possible_tables[file].append(col)\n",
    "            else:\n",
    "                for a in input_sch_onto:\n",
    "                    if (col in input_sch_onto[a]) and (input_schema[a]==d_type):\n",
    "                        if file in possible_tables:\n",
    "                            possible_tables[file].append(a)\n",
    "                        else:\n",
    "                            possible_tables[file]=[]\n",
    "                            possible_tables[file].append(a)\n",
    "    return possible_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if input has categories as well as schema(columns and their dataypes)\n",
    "# we consider it a match under the assumption that at least 75% category match exists\n",
    "def cat_and_col_list(input_categories,input_cat_onto,input_schema,input_sch_onto):\n",
    "    possible_tables={}\n",
    "    for file in all_categories:\n",
    "        cat_list=[]\n",
    "        category=all_categories[file]\n",
    "        for cat in category:\n",
    "            if cat in input_categories:\n",
    "                cat_list.append(cat)\n",
    "            else:\n",
    "                for cat1 in input_cat_onto:\n",
    "                    if cat in input_cat_onto[cat1]:\n",
    "                        cat_list.append(cat1)\n",
    "        cos_val=cos_sim(cat_list,input_categories)\n",
    "        if cos_val > 0.75 :\n",
    "            schema=all_schemas[file]\n",
    "            for col,d_type in schema.items():\n",
    "                if (col in input_schema) and (input_schema[col]==d_type):\n",
    "                    if file in possible_tables:\n",
    "                        possible_tables[file].append(col)\n",
    "                    else:\n",
    "                        possible_tables[file]=[]\n",
    "                        possible_tables[file].append(col)\n",
    "                else:\n",
    "                    for a in input_sch_onto:\n",
    "                        if (col in input_sch_onto[a]) and (input_schema[a]==d_type):\n",
    "                            if file in possible_tables:\n",
    "                                possible_tables[file].append(a)\n",
    "                            else:\n",
    "                                possible_tables[file]=[]\n",
    "                                possible_tables[file].append(a)\n",
    "    return possible_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the given input file names , find the columns that match if any.\n",
    "#If no colmns match , then don't do anything.\n",
    "#If columns match , then check if the columns are present in the list of candidate keys.\n",
    "#If not present in list of candidate keys , just check data.\n",
    "#If present in list of candidate keys and data of those columns match , append the others columns to any of the table to extend the table schema.\n",
    "#If present in list of candidate keys but data dont match , then increse volume by adding the rows to one of the table , but will create a lot of NULL values. Undesirable.\n",
    "#If all the columns match , then to each table just add another column with the table name and append the two tables.\n",
    "\n",
    "def merge(fname1,fname2):\n",
    "    matching_columns = {}  #resultant list containing the matching columns.\n",
    "    #getting schema for tables.\n",
    "    f1 = all_schemas[fname1]\n",
    "    f2 = all_schemas[fname2]\n",
    "    #Retrieving the column names and generating the ontologies for one of the tables columns\n",
    "    f1_cols = list(f1)\n",
    "    f1_onto = generate_list_ontology(f1_cols)\n",
    "    #finding columns that match.     \n",
    "    for col , d_type in f2.items():\n",
    "        if(col in f1_cols):\n",
    "            matching_columns[col] = col\n",
    "        else:\n",
    "            for col_t1 in f1_onto:\n",
    "                if(col in f1_onto[col_t1]):\n",
    "                    matching_columns[col_t1] = col\n",
    "                    break\n",
    "    #Now the varaible matching_columns contains a list of names of columns that match between the two tables.\n",
    "    t1 = cand_key[fname1]\n",
    "    t2 = cand_key[fname2]\n",
    "    mat_cols={}\n",
    "    for key , value in matching_columns.items():\n",
    "        if(key in t1 or value in t2):\n",
    "            mat_cols[key]=value\n",
    "    return merge_tables(fname1,fname2,mat_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tables(fname1,fname2,cols):\n",
    "    t1 = pd.read_csv(fname1)\n",
    "    t2 = pd.read_csv(fname2)\n",
    "    print(fname1+' and '+fname2+' gives : ')\n",
    "    l=len(cols)\n",
    "    if l!=0:\n",
    "        for name1,name2 in cols.items():\n",
    "            t2.rename(columns = {name2:name1},inplace=True)\n",
    "    t3=t1.merge(t2,how='outer')\n",
    "    display(t3)\n",
    "    return t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_possible_matches():\n",
    "    with open(\"input.txt\",'r') as ip_file:\n",
    "        json_object=json.load(ip_file)\n",
    "    input_schema=json_object[\"schema\"]\n",
    "    input_sch_onto=generate_schema_ontology(input_schema)\n",
    "    if \"categories\" in json_object:\n",
    "        print('category and schema')\n",
    "        input_categories=json_object[\"categories\"]\n",
    "        input_cat_onto=generate_list_ontology(input_categories)\n",
    "        possible_tables=cat_and_col_list(input_categories,input_cat_onto,input_schema,input_sch_onto)\n",
    "    else:\n",
    "        print('only schema')\n",
    "        possible_tables=col_only_list(input_schema,input_sch_onto)\n",
    "    matching_tables=[]\n",
    "    for i in possible_tables:\n",
    "        cos_val=cos_sim(possible_tables[i],list(input_schema))\n",
    "        if cos_val>0.75:\n",
    "            matching_tables.append(i)\n",
    "    return matching_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches():\n",
    "    matching_tables=check_possible_matches()\n",
    "    op_str1='output_folder/'\n",
    "    op_str2='.csv'\n",
    "    f=open(\"output_folder/results.txt\",'w')\n",
    "    x=len(matching_tables)\n",
    "    print(matching_tables)\n",
    "    if x==0:\n",
    "        print(\"NO MATCHES FOUND\",file=f)\n",
    "    elif x==1:\n",
    "        print(matching_tables[0],file=f)\n",
    "    else:\n",
    "        ctr=0\n",
    "        for i in range(x-1):\n",
    "            for j in range(i+1,x):\n",
    "                a=matching_tables[i]+' : '+matching_tables[j]\n",
    "                b=matching_tables[j]+' : '+matching_tables[i]\n",
    "                if (a in cat_sim):\n",
    "                    if cat_sim[a]>=.50 and col_sim[a]>=0.55:\n",
    "                        res=merge(matching_tables[i],matching_tables[j])\n",
    "                        ctr+=1\n",
    "                        op_string=op_str1+str(ctr)+op_str2\n",
    "                        res.to_csv(op_string,sep=',', index=False)\n",
    "                        print(matching_tables[i]+' and '+matching_tables[j],file=f)\n",
    "                elif (b in cat_sim):\n",
    "                    if cat_sim[b]>=.75 and col_sim[b]>=0.50:\n",
    "                        res=merge(matching_tables[i],matching_tables[j])\n",
    "                        ctr+=1\n",
    "                        op_string=op_str1+str(ctr)+op_str2\n",
    "                        res.to_csv(op_string,sep=',', index=False)\n",
    "                        print(matching_tables[i]+' and '+matching_tables[j],file=f)\n",
    "                else:\n",
    "                    pass\n",
    "        for i in matching_tables:\n",
    "            print(i,file=f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category and schema\n",
      "['202_117.csv', '203_268.csv']\n",
      "202_117.csv and 203_268.csv gives : \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ship</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>Tonnage</th>\n",
       "      <th>Fate</th>\n",
       "      <th>Name</th>\n",
       "      <th>Tonnage (GRT)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19 August 1941</td>\n",
       "      <td>SS Aguila</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>3255.0</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27 November 1941</td>\n",
       "      <td>HMAS Parramatta</td>\n",
       "      <td>Royal Australian Navy</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23 December 1941</td>\n",
       "      <td>SS Shuntien</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>3059.0</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26 December 1941</td>\n",
       "      <td>SS Warszawa</td>\n",
       "      <td>Poland</td>\n",
       "      <td>2487.0</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10 June 1942</td>\n",
       "      <td>MV Athene</td>\n",
       "      <td>Norway</td>\n",
       "      <td>4681.0</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10 June 1942</td>\n",
       "      <td>SS Brambleleaf</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>5917.0</td>\n",
       "      <td>Damaged</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7 October 1941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damaged</td>\n",
       "      <td>Svend Foyn</td>\n",
       "      <td>14795.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16 February 1942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Venezuela</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>Monagas</td>\n",
       "      <td>2650.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16 February 1942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>San Nicholas</td>\n",
       "      <td>2391.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16 February 1942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>Tia Juana</td>\n",
       "      <td>2395.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>22 February 1942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>J.N.Pew</td>\n",
       "      <td>9033.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>23 February 1942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damaged</td>\n",
       "      <td>Sun</td>\n",
       "      <td>9002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23 February 1942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Panama</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>Thalia</td>\n",
       "      <td>8329.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11 May 1942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>Cape of Good Hope</td>\n",
       "      <td>4963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24 May 1942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brazil</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>Gonçalves Dias</td>\n",
       "      <td>4996.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>28 May 1942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>Alcoa Pilgrim</td>\n",
       "      <td>6759.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3 June 1942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>M.F. Ellliot</td>\n",
       "      <td>6940.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9 June 1942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Belgium</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>Bruxelles</td>\n",
       "      <td>5085.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9 June 1942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>Franklin K. Lane</td>\n",
       "      <td>6589.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>15 June 1942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Panama</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>Cold Harbor</td>\n",
       "      <td>5010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15 June 1942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>Scottsburg</td>\n",
       "      <td>8010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15 June 1942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunk</td>\n",
       "      <td>West Hardaway</td>\n",
       "      <td>5702.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Date             Ship            Nationality  Tonnage  \\\n",
       "0     19 August 1941        SS Aguila         United Kingdom   3255.0   \n",
       "1   27 November 1941  HMAS Parramatta  Royal Australian Navy   1060.0   \n",
       "2   23 December 1941      SS Shuntien         United Kingdom   3059.0   \n",
       "3   26 December 1941      SS Warszawa                 Poland   2487.0   \n",
       "4       10 June 1942        MV Athene                 Norway   4681.0   \n",
       "5       10 June 1942   SS Brambleleaf         United Kingdom   5917.0   \n",
       "6     7 October 1941              NaN         United Kingdom      NaN   \n",
       "7   16 February 1942              NaN              Venezuela      NaN   \n",
       "8   16 February 1942              NaN         United Kingdom      NaN   \n",
       "9   16 February 1942              NaN         United Kingdom      NaN   \n",
       "10  22 February 1942              NaN          United States      NaN   \n",
       "11  23 February 1942              NaN          United States      NaN   \n",
       "12  23 February 1942              NaN                 Panama      NaN   \n",
       "13       11 May 1942              NaN         United Kingdom      NaN   \n",
       "14       24 May 1942              NaN                 Brazil      NaN   \n",
       "15       28 May 1942              NaN          United States      NaN   \n",
       "16       3 June 1942              NaN          United States      NaN   \n",
       "17       9 June 1942              NaN                Belgium      NaN   \n",
       "18       9 June 1942              NaN          United States      NaN   \n",
       "19      15 June 1942              NaN                 Panama      NaN   \n",
       "20      15 June 1942              NaN          United States      NaN   \n",
       "21      15 June 1942              NaN          United States      NaN   \n",
       "\n",
       "       Fate               Name  Tonnage (GRT)  \n",
       "0      Sunk                NaN            NaN  \n",
       "1      Sunk                NaN            NaN  \n",
       "2      Sunk                NaN            NaN  \n",
       "3      Sunk                NaN            NaN  \n",
       "4      Sunk                NaN            NaN  \n",
       "5   Damaged                NaN            NaN  \n",
       "6   Damaged         Svend Foyn        14795.0  \n",
       "7      Sunk            Monagas         2650.0  \n",
       "8      Sunk       San Nicholas         2391.0  \n",
       "9      Sunk          Tia Juana         2395.0  \n",
       "10     Sunk            J.N.Pew         9033.0  \n",
       "11  Damaged                Sun         9002.0  \n",
       "12     Sunk             Thalia         8329.0  \n",
       "13     Sunk  Cape of Good Hope         4963.0  \n",
       "14     Sunk     Gonçalves Dias         4996.0  \n",
       "15     Sunk      Alcoa Pilgrim         6759.0  \n",
       "16     Sunk       M.F. Ellliot         6940.0  \n",
       "17     Sunk          Bruxelles         5085.0  \n",
       "18     Sunk   Franklin K. Lane         6589.0  \n",
       "19     Sunk        Cold Harbor         5010.0  \n",
       "20     Sunk         Scottsburg         8010.0  \n",
       "21     Sunk      West Hardaway         5702.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_matches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
