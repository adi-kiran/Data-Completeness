{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries and packages\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "from itertools import combinations\n",
    "from nltk.corpus import wordnet\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preloading all schemas and categories\n",
    "all_schemas={}\n",
    "all_categories={}\n",
    "all_tablenames={}\n",
    "with open(\"final_schema.txt\") as ip_file:\n",
    "    for line in ip_file.readlines():\n",
    "        json_obj=json.loads(line)\n",
    "        all_schemas[json_obj[\"filename\"]]=json_obj[\"schema\"]\n",
    "        all_categories[json_obj[\"filename\"]]=json_obj[\"categories\"]\n",
    "        all_tablenames[json_obj[\"filename\"]]=json_obj[\"tablename\"]\n",
    "#preloading column and category similarity values of tables\n",
    "with open(\"cos_similarity.txt\",'r') as ip_file:\n",
    "    json_object=json.load(ip_file)\n",
    "    col_sim = json_object[\"column_similarity\"]\n",
    "    cat_sim = json_object[\"category_similarity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all functions needed to generate ontologies\n",
    "\n",
    "# generates all synonyms and hypernyms of a term using wordnet\n",
    "def get_synonyms(word):\n",
    "    meanings=set()\n",
    "    for synset in wordnet.synsets(word,pos=wordnet.NOUN):\n",
    "        for lemma in synset.lemmas():\n",
    "            meanings.add(lemma.name())\n",
    "    for synset in wordnet.synsets(word,pos=wordnet.NOUN):\n",
    "        for hypernym in synset.hypernyms():\n",
    "            meanings.add(hypernym.lemma_names()[0])\n",
    "    meanings.add(word)\n",
    "    return list(meanings)\n",
    "\n",
    "# takes input list and returns ontology as dictionary with every word in list as the key\n",
    "def generate_ontology(list1):\n",
    "    ontology={}\n",
    "    for word in list1:\n",
    "        ontology[word]=get_synonyms(word)\n",
    "    return ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list of transformation functions\n",
    "tf_onto={}\n",
    "transform_funct_list={\"int64\":['average','sum','maximum','minimum','range','median','variance','standard deviation','mode','frequency','avg'],\"float64\":['average','sum','maximum','minimum','range','median','variance','standard deviation','mode','frequency','avg'],\"object\":[\"tolower\",\"toupper\",\"substring\",\"funct1\",\"funct2\",\"funct3\"]}\n",
    "for dtype,funct_list in transform_funct_list.items():\n",
    "    tf_onto[dtype]=generate_ontology(funct_list)\n",
    "# initialising knowledge graph data dictionary\n",
    "kg_data={}\n",
    "# making a list of all statistical functions and the various signs and combinations \n",
    "signs = [\"<=\",\">=\",\"<\",\">\",\"=\",\"!=\"]; functs =  ['count','mean','standard deviation','min','max','25%','50%','75%']; functs_onto = generate_ontology(functs)\n",
    "functs_onto['25%'].append(\"first quartile\");functs_onto['75%'].append(\"third quartile\");functs_onto['50%'].append(\"second quartile\");functs_onto['min'].append(\"minimum\");functs_onto['max'].append(\"maximum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate cos similarity between two lists\n",
    "def cos_sim(list1, list2):\n",
    "    terms = set(list1).union(list2)\n",
    "    intersect = set(list1) & set(list2)\n",
    "    others = (set(list1)-intersect).union(set(list2)-intersect)\n",
    "    product=0\n",
    "    for word in terms:\n",
    "        if word in intersect:\n",
    "            product+=1\n",
    "    l1mag = math.sqrt(len(list1))\n",
    "    l2mag = math.sqrt(len(list2))\n",
    "    if len(list1)==0 or len(list2)==0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return product / (l1mag * l2mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dictionary of all tables having one or more columns mstching directly with the input schema\n",
    "def direct_matches(input_schema):\n",
    "    possible_tables={}\n",
    "    for file in all_schemas:\n",
    "        schema=all_schemas[file]\n",
    "        cols={col1:col1 for col1,d_type1 in schema.items() if col1 in input_schema and d_type1==input_schema[col1]}\n",
    "        if(len(cols)>1):\n",
    "            possible_tables[file]=cols\n",
    "    return possible_tables\n",
    "\n",
    "# returns a dictionary of all tables having one or more columns mstching directly, or having an ontology match \n",
    "# or a substring match with the input schema\n",
    "def extended_matches(input_schema):\n",
    "    possible_tables={}\n",
    "    ip_schema=input_schema\n",
    "    for file in all_schemas:\n",
    "        schema=all_schemas[file]\n",
    "        cols1={col1:col1 for col1,d_type1 in schema.items() if col1 in input_schema and d_type1==input_schema[col1]}\n",
    "        schema={k:v for k,v in schema.items() if k not in cols1}\n",
    "        input_schema={k:v for k,v in input_schema.items() if k not in list(cols1.values())}\n",
    "        input_sch_onto=generate_ontology(input_schema)\n",
    "        cols2={col1:col for col1,d_type1 in schema.items() for col in input_sch_onto if col1 in input_sch_onto[col] and d_type1==input_schema[col]}\n",
    "        schema={k:v for k,v in schema.items() if k not in cols2}\n",
    "        input_schema={k:v for k,v in input_schema.items() if k not in list(cols2.values())}\n",
    "        schema_onto=generate_ontology(schema)\n",
    "        cols3={col:col1 for col1,d_type1 in input_schema.items() for col in schema_onto if col1 in schema_onto[col] and d_type1==schema[col]}\n",
    "        schema={k:v for k,v in schema.items() if k not in cols3}\n",
    "        input_schema={k:v for k,v in input_schema.items() if k not in list(cols3.values())}\n",
    "        cols={**cols1,**cols2,**cols3}\n",
    "        for col1,d_type1 in input_schema.items():\n",
    "            x=[col2 for col2,d_type2 in schema.items() if ((len(col1)>3 and len(col2)>3) and ((col1 in col2) or (col2 in col1))) and (d_type1==d_type2)]\n",
    "            if len(x)==1:\n",
    "                cols[x[0]]=col1\n",
    "        if(len(cols)>1):\n",
    "            possible_tables[file]=cols\n",
    "        input_schema=ip_schema\n",
    "    return possible_tables\n",
    "\n",
    "# categories provided as input help narrow down the matches to contextually relevant tables\n",
    "# categories and columns are matched based on direct match as well as ontology match\n",
    "def categories_included(input_categories,input_schema):\n",
    "    input_cat_onto=generate_ontology(input_categories)\n",
    "    ip_schema=input_schema\n",
    "    possible_tables={}\n",
    "    for file in all_categories:\n",
    "        category=all_categories[file]\n",
    "        cats1=[cat1 for cat1 in category if cat1 in input_categories]\n",
    "        category=[k for k in category if k not in cats1]\n",
    "        cats2=[cat2 for cat1 in category for cat2 in input_cat_onto if cat1 in input_cat_onto[cat2]]\n",
    "        cat_list=cats1+cats2\n",
    "        cos_val=cos_sim(cat_list,input_categories)\n",
    "        if cos_val > 0.75 :\n",
    "            schema=all_schemas[file]\n",
    "            cols1={col1:col1 for col1,d_type1 in schema.items() if col1 in input_schema and d_type1==input_schema[col1]}\n",
    "            schema={k:v for k,v in schema.items() if k not in list(cols1.values())}\n",
    "            input_schema={k:v for k,v in input_schema.items() if k not in list(cols1.values())}\n",
    "            input_sch_onto=generate_ontology(input_schema)\n",
    "            cols2={col1:col for col1,d_type1 in schema.items() for col in input_sch_onto if col1 in input_sch_onto[col] and d_type1==input_schema[col]}\n",
    "            schema={k:v for k,v in schema.items() if k not in cols2}\n",
    "            input_schema={k:v for k,v in input_schema.items() if k not in list(cols2.values())}\n",
    "            schema_onto=generate_ontology(schema)\n",
    "            cols3={col:col1 for col1,d_type1 in input_schema.items() for col in schema_onto if col1 in schema_onto[col] and d_type1==schema[col]}\n",
    "            schema={k:v for k,v in schema.items() if k not in cols3}\n",
    "            input_schema={k:v for k,v in input_schema.items() if k not in list(cols3.values())}\n",
    "            cols={**cols1,**cols2,**cols3}\n",
    "            for col1,d_type1 in input_schema.items():\n",
    "                x=[col2 for col2,d_type2 in schema.items() if ((len(col1)>3 and len(col2)>3) and ((col1 in col2) or (col2 in col1))) and (d_type1==d_type2)]\n",
    "                if len(x)==1:\n",
    "                    cols[x[0]]=col1\n",
    "            if(len(cols)>1):\n",
    "                possible_tables[file]=cols\n",
    "            input_schema=ip_schema\n",
    "    return possible_tables\n",
    "\n",
    "# return only those tables having 75% cos similarity with input table through direct match\n",
    "def direct_matches_only():\n",
    "    input_sch_onto=generate_ontology(input_schema)\n",
    "    possible_tables=direct_matches(input_schema)\n",
    "    matching_tables={}\n",
    "    for i in possible_tables:\n",
    "        cos_val=cos_sim(list(possible_tables[i].values()),list(input_schema))\n",
    "        if cos_val>0.75:\n",
    "            matching_tables[i]=possible_tables[i]\n",
    "    print(\"Matching Tables \")\n",
    "    for i in list(matching_tables):\n",
    "        print(i,\" : \",all_tablenames[i])\n",
    "    print(\"\\n\")\n",
    "    display_individual_matches(list(matching_tables),matching_tables)\n",
    "\n",
    "# return only those tables having 75% cos similarity with input table through direct, ontology and substring match\n",
    "def check_possible_extended_matches():\n",
    "    input_sch_onto=generate_ontology(input_schema)\n",
    "    possible_tables=extended_matches(input_schema)\n",
    "    matching_tables={}\n",
    "    for i in possible_tables:\n",
    "        cos_val=cos_sim(list(possible_tables[i].values()),list(input_schema))\n",
    "        if cos_val>0.75:\n",
    "            matching_tables[i]=possible_tables[i]\n",
    "    print(\"Matching Tables\")\n",
    "    for i in list(matching_tables):\n",
    "        print(i,\" : \",all_tablenames[i])\n",
    "    print(\"\\n\")\n",
    "    display_individual_matches(list(matching_tables),matching_tables)\n",
    "    return matching_tables\n",
    "\n",
    "# return tables having 75% column as well as category cos similarity\n",
    "def check_possible_category_included_matches():\n",
    "    input_sch_onto=generate_ontology(input_schema)\n",
    "    possible_tables=categories_included(input_categories,input_schema)\n",
    "    matching_tables={}\n",
    "    for i in possible_tables:\n",
    "        cos_val=cos_sim(list(possible_tables[i].values()),list(input_schema))\n",
    "        if cos_val>0.75:\n",
    "            matching_tables[i]=possible_tables[i]\n",
    "    print(\"Matching Tables\")\n",
    "    for i in list(matching_tables):\n",
    "        print(i,\" : \",all_tablenames[i])\n",
    "    print(\"\\n\")\n",
    "    display_individual_matches(list(matching_tables),matching_tables)\n",
    "    return matching_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function generates all possible combinations of list l taking elements n to 2 at a time and returns a dictionary\n",
    "def generate_all_combinations(l):\n",
    "    x={}\n",
    "    a=len(l)\n",
    "    for i in range(a,1,-1):\n",
    "        x[i]=list(combinations(l,i))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input table and column, get a list of all categories that the terms(elements) in that column belong to\n",
    "def extract_info_from_knowledge_graphs(a,col1):\n",
    "    with open(\"C:\\\\Users\\\\adith\\\\Desktop\\\\my_google_knowledge_graph_api_key.txt\",\"r\") as f:\n",
    "        api_key=f.readline()\n",
    "    service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
    "    params = {'limit': 50,'indent': True,'key': api_key}\n",
    "    s={}\n",
    "    ctr=0\n",
    "    for row in a[col1]:\n",
    "        if type(row)==type(np.nan):\n",
    "            ctr+=1\n",
    "            continue\n",
    "        params['query']=row\n",
    "        url = service_url + '?' + urllib.parse.urlencode(params)\n",
    "        try:\n",
    "            response = json.loads(urllib.request.urlopen(url).read())\n",
    "        except urllib.error.HTTPError as httperr:\n",
    "            try:\n",
    "                response = json.loads(urllib.request.urlopen(url).read())\n",
    "            except urllib.error.HTTPError as httperr:\n",
    "                try:\n",
    "                    response = json.loads(urllib.request.urlopen(url).read())\n",
    "                except urllib.error.HTTPError as httperr:\n",
    "                    ctr+=1\n",
    "        for element in response['itemListElement']:\n",
    "            for i in element[\"result\"][\"@type\"]:\n",
    "                b=i.lower()\n",
    "                if b in s:\n",
    "                    s[b]+=1\n",
    "                else:\n",
    "                    s[b]=1\n",
    "            if \"description\" in element[\"result\"]:\n",
    "                b=element[\"result\"][\"description\"].lower()\n",
    "                if b in s:\n",
    "                    s[b]+=1\n",
    "                else:\n",
    "                    s[b]=1\n",
    "    return s,ctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using data from google knowledge graphs, try to obtain alternate names for columns that do not match to merge them if they\n",
    "# represent the same entity\n",
    "# returned values are in the form (final_name,old_name)\n",
    "def get_alternate_col_name(a,b,col1,col2):\n",
    "    global kg_data\n",
    "    if col1 not in kg_data:\n",
    "        groups,count=extract_info_from_knowledge_graphs(a,col1)\n",
    "        kg_data[col1]={\"groups\":groups,\"count\":count}\n",
    "    if col2 not in kg_data:\n",
    "        groups,count=extract_info_from_knowledge_graphs(b,col2)\n",
    "        kg_data[col2]={\"groups\":groups,\"count\":count}\n",
    "    s1=kg_data[col1][\"groups\"]\n",
    "    ctr1=kg_data[col1][\"count\"]\n",
    "    s2=kg_data[col2][\"groups\"]\n",
    "    ctr2=kg_data[col2][\"count\"]\n",
    "    if (col1 in s1) and (col1 in s2):\n",
    "        if (s1[col1]>=len(a)-1-ctr1) and (s2[col1]>=len(b)-1-ctr2):\n",
    "            kg_data[col1][\"groups\"][col1]+=kg_data[col2][\"groups\"][col1]\n",
    "            kg_data[col1][\"count\"]+=kg_data[col2][\"count\"]\n",
    "            del kg_data[col2]\n",
    "            return (col1,col2)\n",
    "    elif (col2 in s1) and (col2 in s2):\n",
    "        if (s1[col2]>=len(a)-1-ctr1) and (s2[col2]>=len(b)-1-ctr2):\n",
    "            kg_data[col2][\"groups\"][col2]+=kg_data[col1][\"groups\"][col2]\n",
    "            kg_data[col2][\"count\"]+=kg_data[col1][\"count\"]\n",
    "            del kg_data[col1]\n",
    "            return (col2,col1)\n",
    "    else:\n",
    "        return 0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using google knowledge graph update the column names and update tables (if possible) to improve merge accuracy and improve completeness\n",
    "def graph_match(c,t1,t2):\n",
    "    l=list(combinations(c,2))\n",
    "    possible_column_renames={}\n",
    "    flag=0\n",
    "    for i in l:\n",
    "        if ((i[0] in t1) and (i[1] in t2)):\n",
    "            a,b=get_alternate_col_name(t1,t2,i[0],i[1])\n",
    "        elif ((i[1] in t1) and (i[0] in t2)):\n",
    "            a,b=get_alternate_col_name(t1,t2,i[1],i[0])\n",
    "        else:\n",
    "            a,b=0,0\n",
    "        if a!=0 and b!=0:\n",
    "            if b in input_schema:\n",
    "                if b in t1:\n",
    "                    t2.rename(columns={a:b},inplace=True)\n",
    "                else:\n",
    "                    t1.rename(columns={a:b},inplace=True)\n",
    "            else:\n",
    "                if a in t1:\n",
    "                    t2.rename(columns={b:a},inplace=True)\n",
    "                else:\n",
    "                    t1.rename(columns={b:a},inplace=True)\n",
    "            c.remove(a)\n",
    "            c.remove(b)\n",
    "            return t1,t2,c\n",
    "    return t1,t2,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_kg_data(c,a,res):\n",
    "    global kg_data\n",
    "    for col in c:\n",
    "        if col in a:\n",
    "            b=a\n",
    "        else:\n",
    "            b=res\n",
    "        if col not in kg_data:\n",
    "            groups,ctr=extract_info_from_knowledge_graphs(b,col)\n",
    "            kg_data[col]={\"groups\":groups,\"count\":ctr}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function generates all matching columns between the res_cols schema and columns of table in fname\n",
    "def generate_matching_columns(res_cols,fname):\n",
    "    a=all_schemas[fname]\n",
    "    c={**res_cols,**a}\n",
    "    res_onto=generate_ontology(res_cols)\n",
    "    cols1={col1:col1 for col1,d_type1 in a.items() if (col1 in res_cols) and d_type1==res_cols[col1]}\n",
    "    res_onto={k:v for k,v in res_onto.items() if k not in cols1}\n",
    "    a={k:v for k,v in a.items() if k not in cols1}\n",
    "    cols2={col1:cols for col1,d_type1 in a.items() for col in res_onto if col1 in res_onto[col] and d_type1==res_cols[col]}\n",
    "    a={k:v for k,v in a.items() if k not in cols2}\n",
    "    cols={**cols1,**cols2}\n",
    "    res_cols={k:v for k,v in res_cols.items() if k not in list(cols.values())}\n",
    "    for col1,d_type1 in a.items():\n",
    "        x=[col2 for col2,d_type2 in res_cols.items() if ((len(col1)>3 and len(col2)>3) and ((col1 in col2) or (col2 in col1))) and (d_type1==d_type2)]\n",
    "        if len(x)==1:\n",
    "            cols[col1]=x[0]\n",
    "    matching_columns=cols\n",
    "    c={k:v for k,v in c.items() if k not in matching_columns and v=='object'}\n",
    "    return (matching_columns,list(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a merged table of all tables given in input list l    \n",
    "def merge_list_knowledge_graph(l):\n",
    "    t1=pd.read_csv(l[0])\n",
    "    a=all_schemas[l[0]]\n",
    "    matching_columns,c=generate_matching_columns(a,l[1])\n",
    "    t2=pd.read_csv(l[1])\n",
    "    gen_kg_data(c,t1,t2)\n",
    "    if len(c)>=2:\n",
    "        t1,t2,c=graph_match(c,t1,t2)\n",
    "        while c!=0:\n",
    "            t1,t2,c=graph_match(c,t1,t2)\n",
    "    t2.rename(columns = matching_columns,inplace=True)\n",
    "    try:\n",
    "        res=t1.merge(t2,how='outer')\n",
    "    except:\n",
    "        return -1\n",
    "    for fname in l[2:]:\n",
    "        res_cols={i:j for i,j in zip(res.columns,[str(i) for i in res.dtypes])}\n",
    "        matching_columns,c=generate_matching_columns(res_cols,fname)\n",
    "        t=pd.read_csv(fname)\n",
    "        gen_kg_data(c,res,t)\n",
    "        if len(c)>=2:\n",
    "            res,t,c=graph_match(c,res,t)\n",
    "            while c!=0:\n",
    "                res,t,c=graph_match(c,res,t)\n",
    "        t.rename(columns = matching_columns,inplace=True)\n",
    "        try:\n",
    "            res=res.merge(t,how='outer')\n",
    "        except:\n",
    "            return -1\n",
    "    res_cols={i:j for i,j in zip(res.columns,[str(i) for i in res.dtypes])}\n",
    "    a=input_schema\n",
    "    cols1={col1:col1 for col1,d_type1 in res_cols.items() if (col1 in a) and d_type1==input_schema[col1]}\n",
    "    a={k:v for k,v in a.items() if k not in cols1}\n",
    "    a_onto=generate_ontology(a)\n",
    "    cols2={col1:col for col1,d_type1 in res_cols.items() for col in a_onto if col1 in a_onto[col] and (d_type1==a[col])}\n",
    "    a={k:v for k,v in a.items() if k not in cols2}\n",
    "    cols={**cols1,**cols2}\n",
    "    res_cols={k:v for k,v in res_cols.items() if k not in list(cols.values())}\n",
    "    for col1,d_type1 in a.items():\n",
    "        x=[col2 for col2,d_type2 in res_cols.items() if ((len(col1)>3 and len(col2)>3) and ((col1 in col2) or (col2 in col1))) and (d_type1==d_type2)]\n",
    "        if len(x)==1:\n",
    "            cols[x[0]]=col1\n",
    "    res.rename(columns = cols,inplace=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics calculated :\n",
    "# 1)nan_score(number of nulls in each column)\n",
    "# 2)coverage_score(no of matching columns with input schema/total number of columns in input schema)\n",
    "# 3)completeness_score(a combination of coverage and nan scores to determine how complete the result dataset is)\n",
    "\n",
    "# nan score = {x : (no on nans in column/no of entries in column)} where x is each column in the table\n",
    "# gives the nan score(no on nans/no of entries in table) for each column in the input table\n",
    "def nan_score(table=-1,fname=-1):\n",
    "    if fname!=-1:\n",
    "        table=pd.read_csv(fname)\n",
    "    nan_count={}\n",
    "    a=len(table)\n",
    "    for i in table.columns:\n",
    "        x=a-table[i].count()\n",
    "        s=str(x)+'/'+str(a)\n",
    "        nan_count[i]=s\n",
    "    return nan_count\n",
    "\n",
    "#returns the coverage score and completeness score of a given table\n",
    "#coverage score is calculated as : \n",
    "# coverage = (no of columns matching with input schema/total number of columns in input schema)\n",
    "#completeness score is calculated as : \n",
    "# completeness = (sum(x*(non null entries)/(total entries in the column))/total number of columns in input schema) \n",
    "#  where x=1 if column present in input schema and x=0 if column is not present in the input schema\n",
    "def coverage_and_completeness(table):\n",
    "    ctr=0\n",
    "    comp=0.0\n",
    "    cols=table.columns\n",
    "    l=len(table)\n",
    "    for col in cols:\n",
    "        if col in input_schema:\n",
    "            ctr+=1\n",
    "            comp+=((l-sum(pd.isnull(table[col])))/l)\n",
    "    comp=comp/len(input_schema)\n",
    "    cov=ctr/len(input_schema)\n",
    "    return (cov,comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_display(comp_score,no_of_rows):\n",
    "    print()\n",
    "    comp_rank=sorted(comp_score,reverse=True)\n",
    "    count=0\n",
    "    l=sorted(no_of_rows, key=lambda k: no_of_rows[k],reverse=True)\n",
    "    for i in comp_rank:\n",
    "        if len(comp_score[i])==1:\n",
    "            count+=1\n",
    "            print(\"Rank \",str(count).ljust(2,\" \"),\" : \",comp_score[i][0].ljust(20,' '),\"completeness score : %0.16f\"%(i),\"\\t\\tnumber of rows: \",no_of_rows[comp_score[i][0]])\n",
    "        else:\n",
    "            for j in l:\n",
    "                if j in comp_score[i]:\n",
    "                    count+=1\n",
    "                    print(\"Rank \",str(count).ljust(2,\" \"),\" : \",j.ljust(20,' '),\"completeness score : %0.16f\"%(i),\"\\t\\tnumber of rows: \",no_of_rows[j])\n",
    "    print(\"\\nRanking Complete!!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes output tables schema(columns and data_types) as the input, compare it with input schema and transformations required and\n",
    "# returns a list of all the transformations applicable\n",
    "def get_possible_transformations(cols):\n",
    "    return {col:[i for i in tran if i in transform_funct_list[input_schema[col]]] for col,tran in transformations.items() if col in cols}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returns a boolean result after checking if confition is satisfied\n",
    "def condition_check(sign,v,value):\n",
    "    x={\">=\":v>=value,\"<=\":v<=value,\"<\":v<value,\">\":v>value,\"=\":v==value,\"!=\":v!=value}\n",
    "    return x[sign]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function returns all stastical conditions that are satisfied as well as unsatisfied\n",
    "def check_stats(res):\n",
    "    cols=res.columns; satisfied={}; unsatisfied={}\n",
    "    for col in stats:\n",
    "        if col in cols:\n",
    "            if input_schema[col]==\"int64\" or input_schema[col]==\"float64\":\n",
    "                for condition in stats[col]:\n",
    "                    sign=[i for i in signs if i in condition][0]\n",
    "                    funct,value=condition.split(sign)[0],condition.split(sign)[1]\n",
    "                    funct=funct.lower()\n",
    "                    if funct in functs_onto:\n",
    "                        pass\n",
    "                    else:\n",
    "                        for f in functs_onto:\n",
    "                            if funct in functs_onto[f]:\n",
    "                                funct=f\n",
    "                    v=res[col].describe()[funct]\n",
    "                    if condition_check(sign,v,int(value)):\n",
    "                        s=condition+\"( \"+str(v)+sign+value+\" )\"\n",
    "                        if col in satisfied:\n",
    "                            satisfied[col].append(s)\n",
    "                        else:\n",
    "                            satisfied[col]=[]\n",
    "                            satisfied[col].append(s)\n",
    "                    else:\n",
    "                        s=condition+\"( \"+funct+\" = \"+str(v)+\" )\"\n",
    "                        if col in unsatisfied:\n",
    "                            unsatisfied[col].append(s)\n",
    "                        else:\n",
    "                            unsatisfied[col]=[]\n",
    "                            unsatisfied[col].append(s)\n",
    "        else:\n",
    "            unsatisfied[col+\"(Column Not Present)\"]=stats[col]\n",
    "    return (satisfied,unsatisfied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function to print the individual tables names, their nan score, columns that match with input schema, \n",
    "# coverage score, and completeness score along with possible transformations if any\n",
    "def display_individual_matches(matching_tables,matching_tables_dict):\n",
    "    global comp_score\n",
    "    global no_of_rows\n",
    "    f=open(\"output_folder_transformation/results.txt\",'a')\n",
    "    if transformations==-1:\n",
    "        flag=0\n",
    "    else:\n",
    "        flag=1\n",
    "    for i in matching_tables:\n",
    "        print(i+\"(\"+all_tablenames[i]+\")\",file=f)\n",
    "        print(i+\"(\"+all_tablenames[i]+\")\")\n",
    "        res=pd.read_csv(i)\n",
    "        res.rename(columns=matching_tables_dict[i],inplace=True)\n",
    "        cov,comp=coverage_and_completeness(res)\n",
    "        if flag==1:\n",
    "            cols=res.columns\n",
    "            l=get_possible_transformations(cols)\n",
    "            print(\"possible transformations are : \",l,file=f)\n",
    "            print(\"possible transformations are : \",l)\n",
    "            cov,comp=coverage_and_completeness(res)\n",
    "        if stats!=-1:\n",
    "            sat,unsat=check_stats(res)\n",
    "            if len(sat)>0:\n",
    "                print(\"Conditions Satisfied : \",sat,file=f)\n",
    "                print(\"Conditions Satisfied : \",sat)\n",
    "            if len(unsat)>0:\n",
    "                print(\"Conditions NOT SATISFIED : \",unsat,file=f)\n",
    "                print(\"Conditions NOT SATISFIED : \",unsat)\n",
    "        print('Missing Values(NANs score): ',nan_score(fname=i),file=f)\n",
    "        print('Missing Values(NANs score): ',nan_score(fname=i))\n",
    "        print(\"Columns that match with input_schema:\\n \"+i+' : ',matching_tables_dict[i],file=f)\n",
    "        print(\"Columns that match with input_schema:\\n \"+i+' : ',matching_tables_dict[i])\n",
    "        print(\"Coverage Score : \",cov,\"\\t Completeness Score : \",comp,file=f)\n",
    "        print(\"Coverage Score : \",cov,\"\\t Completeness Score : \",comp)\n",
    "        print(file=f)\n",
    "        print()\n",
    "        no_of_rows[i]=len(res)\n",
    "        if comp in comp_score:\n",
    "            comp_score[comp].append(i)\n",
    "        else:\n",
    "            comp_score[comp]=[]\n",
    "            comp_score[comp].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main function that is to be invoked and will call all the required functions to obtain the required matches and merges\n",
    "# we have a list with all possible tables that are matches\n",
    "# we generate all combinations of them in order to merge them\n",
    "# a valid combination is one where every pair of tables have 50% cosine column and category similarity\n",
    "# then we merge them and calculate the coverage score, null score and the completeness score\n",
    "# all these details are displayed for each of the valid merges as well as individual tables\n",
    "# the output is displayed in the output file \"results.txt\" along with the outputs in csv form\n",
    "def get_matches():\n",
    "    global comp_score\n",
    "    global no_of_rows\n",
    "    comp_score={}\n",
    "    no_of_rows={}\n",
    "    if input_categories==-1:\n",
    "        print('only schema')\n",
    "        matching_tables_dict=check_possible_extended_matches()\n",
    "    else:\n",
    "        print('category and schema')\n",
    "        matching_tables_dict=check_possible_category_included_matches()\n",
    "    matching_tables=list(matching_tables_dict)\n",
    "    with open(\"output_folder_transformation/results.txt\",'a') as f:\n",
    "        print('******************************',file=f)\n",
    "        print(file=f)\n",
    "        print(\"All Possible Matches\",file=f)\n",
    "        print(file=f)\n",
    "        print(\"Matching Tables : \",matching_tables,file=f)\n",
    "        print(\"Matching Tables : \",matching_tables)\n",
    "        for i in matching_tables:\n",
    "            print(i,'\\t',all_tablenames[i])\n",
    "        if transformations==-1:\n",
    "            print(\"\\nNo Transformations In Input Schema\",file=f)\n",
    "            print(file=f)\n",
    "        else:\n",
    "            print('\\nTransformations detected from input are : ',transformations,file=f)\n",
    "            print(file=f)\n",
    "    op_str1='output_folder_transformation/'\n",
    "    op_str2='.csv'\n",
    "    x=len(matching_tables)\n",
    "    print()\n",
    "    if x==0:\n",
    "        with open(\"output_folder_transformation/results.txt\",'a') as f:\n",
    "            print(\"NO MATCHES FOUND\",file=f)\n",
    "            print(file=f)\n",
    "    elif x==1:\n",
    "        display_individual_matches(matching_tables,matching_tables_dict)\n",
    "        print(\"Only one match found!! Rank 1 : \",matching_tables[0])\n",
    "    elif x==2:\n",
    "        a=matching_tables[0]+' : '+matching_tables[1]\n",
    "        b=matching_tables[1]+' : '+matching_tables[0]\n",
    "        if (a in cat_sim) or (b in cat_sim):\n",
    "            if (cat_sim[a]>.50 and col_sim[a]>0.50) or (cat_sim[b]>.50 and col_sim[b]>0.50):\n",
    "                res=merge_list_knowledge_graph([matching_tables[0],matching_tables[1]])\n",
    "                op_string=op_str1+'1'+op_str2\n",
    "                res.to_csv(op_string,sep=',', index=False)\n",
    "                cols=res.columns\n",
    "                cov,comp=coverage_and_completeness(res)\n",
    "                with open(\"output_folder_transformation/results.txt\",'a') as f:\n",
    "                    print(op_string,file=f)\n",
    "                    if transformations!=-1:\n",
    "                        l=get_possible_transformations(cols)\n",
    "                        print(\"possible transformations are : \",l,file=f)\n",
    "                    if stats!=-1:\n",
    "                        sat,unsat=check_stats(res)\n",
    "                        if len(sat)>0:\n",
    "                            print(\"Conditions Satisfied : \",sat,file=f)\n",
    "                        if len(unsat)>0:\n",
    "                            print(\"Conditions Not Satisfied : \",unsat,file=f)\n",
    "                    print('Missing Values(NANs): ',nan_score(table=res),file=f)\n",
    "                    print(\"Columns that match with input_schema: \",file=f)\n",
    "                    for j in matching_tables:\n",
    "                        print(j+' : ',matching_tables_dict[j],file=f)\n",
    "                    print(\"Coverage Score : \",cov,\"\\t Completeness Score : \",comp,\"\\t Number of Rows : \",len(res),file=f)\n",
    "                    print(file=f)\n",
    "                    no_of_rows[\"output_\"+str(ctr)+op_str2]=len(res)\n",
    "                    if comp in comp_score:\n",
    "                        comp_score[comp].append(\"output_\"+str(ctr)+op_str2)\n",
    "                    else:\n",
    "                        comp_score[comp]=[]\n",
    "                        comp_score[comp].append(\"output_\"+str(ctr)+op_str2)\n",
    "        display_individual_matches(matching_tables,matching_tables_dict)\n",
    "    else:\n",
    "        ctr=0\n",
    "        count_comb=len(matching_tables)\n",
    "        all_combos=generate_all_combinations(matching_tables)\n",
    "        for i in range(count_comb,1,-1):\n",
    "            for l in all_combos[i]:\n",
    "                a=list(l)\n",
    "                comb=list(combinations(a,2))\n",
    "                flag=0\n",
    "                for pair in comb:\n",
    "                    if flag==0:\n",
    "                        t1,t2=pair\n",
    "                        if ((t1+' : '+t2) in cat_sim):\n",
    "                            if (cat_sim[t1+' : '+t2]>=.50 and col_sim[t1+' : '+t2]>=0.50):\n",
    "                                pass\n",
    "                            else:\n",
    "                                flag=1\n",
    "                        else:\n",
    "                            flag=1\n",
    "                    else:\n",
    "                        break\n",
    "                if flag==0:\n",
    "                    res=merge_list_knowledge_graph(a)\n",
    "                    if type(res) != type(-1):\n",
    "                        ctr+=1\n",
    "                        op_string=op_str1+str(ctr)+op_str2\n",
    "                        res.to_csv(op_string,sep=',', index=False)\n",
    "                        cols=res.columns\n",
    "                        cov,comp=coverage_and_completeness(res)\n",
    "                        with open(\"output_folder_transformation/results.txt\",'a') as f:\n",
    "                            print(str(ctr)+op_str2+' : ',end='',file=f)\n",
    "                            print(str(ctr)+op_str2+' : ',end='')\n",
    "                            for j in a:\n",
    "                                print(j+'\\t\\t',end='',file=f)\n",
    "                                print(j+'\\t\\t',end='')\n",
    "                            print(file=f)\n",
    "                            print()\n",
    "                            if transformations!=-1:\n",
    "                                l=get_possible_transformations(cols)\n",
    "                                print(\"possible transformations are : \",l,file=f)\n",
    "                                print(\"possible transformations are : \",l)\n",
    "                            if stats!=-1:\n",
    "                                sat,unsat=check_stats(res)\n",
    "                                if len(sat)>0:\n",
    "                                    print(\"Conditions Satisfied : \",sat,file=f)\n",
    "                                    print(\"Conditions Satisfied : \",sat)\n",
    "                                if len(unsat)>0:\n",
    "                                    print(\"Conditions Not Satisfied : \",unsat,file=f)\n",
    "                                    print(\"Conditions Not Satisfied : \",unsat)\n",
    "                            print('Missing Values(NANs): ',nan_score(table=res),file=f)\n",
    "                            print('Missing Values(NANs): ',nan_score(table=res))\n",
    "                            print(\"Columns that match with input_schema: \",file=f)\n",
    "                            print(\"Columns that match with input_schema: \")\n",
    "                            for j in a:\n",
    "                                print(j+' : ',matching_tables_dict[j],file=f)\n",
    "                                print(j+' : ',matching_tables_dict[j])\n",
    "                            print(\"Coverage Score : \",cov,\"\\t Completeness Score : \",comp,file=f)\n",
    "                            print(\"Coverage Score : \",cov,\"\\t Completeness Score : \",comp)\n",
    "                            print()\n",
    "                            print(file=f)\n",
    "                            no_of_rows[\"output_\"+str(ctr)+op_str2]=len(res)\n",
    "                            if comp in comp_score:\n",
    "                                comp_score[comp].append(\"output_\"+str(ctr)+op_str2)\n",
    "                            else:\n",
    "                                comp_score[comp]=[]\n",
    "                                comp_score[comp].append(\"output_\"+str(ctr)+op_str2)\n",
    "        display_individual_matches(matching_tables,matching_tables_dict)\n",
    "    with open(\"output_folder_transformation/results.txt\",'a') as f:\n",
    "        print('******************************',file=f)\n",
    "    ranking_display(comp_score,no_of_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Schema :  {'round': 'int64', 'round 1': 'object', 'circuit': 'object', 'day': 'object', 'pole position': 'object', 'fastest lap': 'object', 'driver': 'object'}\n",
      "Input Categories : None\n",
      "Input Transformations : None\n",
      "Input Condition based conditions : None\n",
      "\n",
      "Matching Tables \n",
      "203_514.csv  :  2008 Superleague Formula season\n",
      "204_253.csv  :  1990 Superbike World Championship season\n",
      "204_40.csv  :  2008 Superbike World Championship season\n",
      "204_569.csv  :  1998 Swedish Touring Car Championship season\n",
      "204_845.csv  :  2003 Barber Dodge Pro Series season\n",
      "\n",
      "\n",
      "203_514.csv(2008 Superleague Formula season)\n",
      "Missing Values(NANs score):  {'round': '0/12', 'round 1': '0/12', 'race': '0/12', 'date': '0/12', 'pole position': '6/12', 'fastest lap': '0/12', 'winning club': '0/12', 'winning team': '0/12', 'report': '0/12'}\n",
      "Columns that match with input_schema:\n",
      " 203_514.csv :  {'round': 'round', 'round 1': 'round 1', 'pole position': 'pole position', 'fastest lap': 'fastest lap'}\n",
      "Coverage Score :  0.5714285714285714 \t Completeness Score :  0.5\n",
      "\n",
      "204_253.csv(1990 Superbike World Championship season)\n",
      "Missing Values(NANs score):  {'round': '0/26', 'round 1': '0/26', 'circuit': '0/26', 'date': '0/26', 'pole position': '0/26', 'fastest lap': '0/26', 'winning rider': '0/26'}\n",
      "Columns that match with input_schema:\n",
      " 204_253.csv :  {'round': 'round', 'round 1': 'round 1', 'circuit': 'circuit', 'pole position': 'pole position', 'fastest lap': 'fastest lap'}\n",
      "Coverage Score :  0.7142857142857143 \t Completeness Score :  0.7142857142857143\n",
      "\n",
      "204_40.csv(2008 Superbike World Championship season)\n",
      "Missing Values(NANs score):  {'round': '0/28', 'round 1': '0/28', 'country': '0/28', 'circuit': '0/28', 'date': '0/28', 'pole position': '0/28', 'fastest lap': '0/28', 'winning rider': '0/28', 'winning team': '0/28', 'report': '0/28'}\n",
      "Columns that match with input_schema:\n",
      " 204_40.csv :  {'round': 'round', 'round 1': 'round 1', 'circuit': 'circuit', 'pole position': 'pole position', 'fastest lap': 'fastest lap'}\n",
      "Coverage Score :  0.7142857142857143 \t Completeness Score :  0.7142857142857143\n",
      "\n",
      "204_569.csv(1998 Swedish Touring Car Championship season)\n",
      "Missing Values(NANs score):  {'round': '0/12', 'round 1': '0/12', 'circuit': '0/12', 'date': '0/12', 'pole position': '6/12', 'fastest lap': '0/12', 'winning driver': '0/12', 'winning team': '0/12', 'winning privateer': '0/12'}\n",
      "Columns that match with input_schema:\n",
      " 204_569.csv :  {'round': 'round', 'round 1': 'round 1', 'circuit': 'circuit', 'pole position': 'pole position', 'fastest lap': 'fastest lap'}\n",
      "Coverage Score :  0.7142857142857143 \t Completeness Score :  0.6428571428571429\n",
      "\n",
      "204_845.csv(2003 Barber Dodge Pro Series season)\n",
      "Missing Values(NANs score):  {'round': '0/10', 'circuit': '0/10', 'location': '0/10', 'date': '0/10', 'pole position': '0/10', 'fastest lap': '0/10', 'winning driver': '0/10', 'headline event': '0/10'}\n",
      "Columns that match with input_schema:\n",
      " 204_845.csv :  {'round': 'round', 'circuit': 'circuit', 'pole position': 'pole position', 'fastest lap': 'fastest lap'}\n",
      "Coverage Score :  0.5714285714285714 \t Completeness Score :  0.5714285714285714\n",
      "\n",
      "\n",
      "Rank  1   :  204_40.csv           completeness score : 0.7142857142857143 \t\tnumber of rows:  28\n",
      "Rank  2   :  204_253.csv          completeness score : 0.7142857142857143 \t\tnumber of rows:  26\n",
      "Rank  3   :  204_569.csv          completeness score : 0.6428571428571429 \t\tnumber of rows:  12\n",
      "Rank  4   :  204_845.csv          completeness score : 0.5714285714285714 \t\tnumber of rows:  10\n",
      "Rank  5   :  203_514.csv          completeness score : 0.5000000000000000 \t\tnumber of rows:  12\n",
      "\n",
      "Ranking Complete!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s='{\"schema\": {\"round\": \"int64\", \"round 1\": \"object\", \"circuit\": \"object\", \"day\": \"object\", \"pole position\": \"object\", \"fastest lap\": \"object\", \"driver\": \"object\"}}'\n",
    "s=s.lower()\n",
    "s=json.loads(s)\n",
    "input_schema=s['schema'];input_categories=-1;transformations=-1;stats=-1\n",
    "if \"categories\" in s:\n",
    "    input_categories=s[\"categories\"]\n",
    "if \"transformations\" in s:\n",
    "    transformations=s[\"transformations\"]\n",
    "if \"stats\" in s:\n",
    "    stats=s[\"stats\"]\n",
    "# display query\n",
    "print(\"Input Schema : \",input_schema)\n",
    "if input_categories==-1:\n",
    "    print(\"Input Categories : None\")\n",
    "else:\n",
    "    print(\"Input Categories : \",input_categories)\n",
    "if transformations==-1:\n",
    "    print(\"Input Transformations : None\")\n",
    "else:\n",
    "    print(\"Input Transformations : \",transformations)\n",
    "if stats==-1:\n",
    "    print(\"Input Condition based conditions : None\")\n",
    "else:\n",
    "    print(\"Input Content based Conditions : \",stats)\n",
    "print()\n",
    "comp_score={}\n",
    "no_of_rows={}\n",
    "no_of_rows={}\n",
    "direct_matches_only()\n",
    "ranking_display(comp_score,no_of_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Schema :  {'round': 'int64', 'round 1': 'object', 'circuit': 'object', 'day': 'object', 'pole position': 'object', 'fastest lap': 'object', 'driver': 'object'}\n",
      "Input Categories : None\n",
      "Input Transformations : None\n",
      "Input Condition based conditions : None\n",
      "\n",
      "Matching Tables\n",
      "203_181.csv  :  1990 IndyCar season\n",
      "203_408.csv  :  1989 Formula One season\n",
      "203_514.csv  :  2008 Superleague Formula season\n",
      "203_742.csv  :  1995 IndyCar season\n",
      "204_253.csv  :  1990 Superbike World Championship season\n",
      "204_40.csv  :  2008 Superbike World Championship season\n",
      "204_455.csv  :  1989 Formula One season\n",
      "204_569.csv  :  1998 Swedish Touring Car Championship season\n",
      "204_63.csv  :  2002 Italian Formula Three season\n",
      "204_845.csv  :  2003 Barber Dodge Pro Series season\n",
      "\n",
      "\n",
      "203_181.csv(1990 IndyCar season)\n",
      "Missing Values(NANs score):  {'date': '0/17', 'rnd': '0/17', 'race name': '0/17', 'circuit': '0/17', 'city/location': '0/17', 'pole position': '0/17', 'winning driver': '0/17', 'winning team': '0/17', 'report': '0/17'}\n",
      "Columns that match with input_schema:\n",
      " 203_181.csv :  {'circuit': 'circuit', 'pole position': 'pole position', 'date': 'day', 'winning driver': 'driver'}\n",
      "Coverage Score :  0.5714285714285714 \t Completeness Score :  0.5714285714285714\n",
      "\n",
      "203_408.csv(1989 Formula One season)\n",
      "Missing Values(NANs score):  {'rd ': '0/16', 'grand prix': '0/16', 'date': '0/16', 'location': '0/16', 'pole position': '0/16', 'fastest lap': '0/16', 'winning driver': '0/16', 'constructor': '0/16', 'report': '0/16'}\n",
      "Columns that match with input_schema:\n",
      " 203_408.csv :  {'pole position': 'pole position', 'fastest lap': 'fastest lap', 'date': 'day', 'winning driver': 'driver'}\n",
      "Coverage Score :  0.5714285714285714 \t Completeness Score :  0.5714285714285714\n",
      "\n",
      "203_514.csv(2008 Superleague Formula season)\n",
      "Missing Values(NANs score):  {'round': '0/12', 'round 1': '0/12', 'race': '0/12', 'date': '0/12', 'pole position': '6/12', 'fastest lap': '0/12', 'winning club': '0/12', 'winning team': '0/12', 'report': '0/12'}\n",
      "Columns that match with input_schema:\n",
      " 203_514.csv :  {'round': 'round', 'round 1': 'round 1', 'pole position': 'pole position', 'fastest lap': 'fastest lap', 'date': 'day'}\n",
      "Coverage Score :  0.7142857142857143 \t Completeness Score :  0.6428571428571429\n",
      "\n",
      "203_742.csv(1995 IndyCar season)\n",
      "Missing Values(NANs score):  {'rnd': '0/17', 'date': '0/17', 'race name': '0/17', 'circuit': '0/17', 'city/location': '0/17', 'pole position': '0/17', 'fastest lap': '0/17', 'winning driver': '0/17', 'winning team': '0/17', 'report': '0/17'}\n",
      "Columns that match with input_schema:\n",
      " 203_742.csv :  {'circuit': 'circuit', 'pole position': 'pole position', 'fastest lap': 'fastest lap', 'date': 'day', 'winning driver': 'driver'}\n",
      "Coverage Score :  0.7142857142857143 \t Completeness Score :  0.7142857142857143\n",
      "\n",
      "204_253.csv(1990 Superbike World Championship season)\n",
      "Missing Values(NANs score):  {'round': '0/26', 'round 1': '0/26', 'circuit': '0/26', 'date': '0/26', 'pole position': '0/26', 'fastest lap': '0/26', 'winning rider': '0/26'}\n",
      "Columns that match with input_schema:\n",
      " 204_253.csv :  {'round': 'round', 'round 1': 'round 1', 'circuit': 'circuit', 'pole position': 'pole position', 'fastest lap': 'fastest lap', 'date': 'day'}\n",
      "Coverage Score :  0.8571428571428571 \t Completeness Score :  0.8571428571428571\n",
      "\n",
      "204_40.csv(2008 Superbike World Championship season)\n",
      "Missing Values(NANs score):  {'round': '0/28', 'round 1': '0/28', 'country': '0/28', 'circuit': '0/28', 'date': '0/28', 'pole position': '0/28', 'fastest lap': '0/28', 'winning rider': '0/28', 'winning team': '0/28', 'report': '0/28'}\n",
      "Columns that match with input_schema:\n",
      " 204_40.csv :  {'round': 'round', 'round 1': 'round 1', 'circuit': 'circuit', 'pole position': 'pole position', 'fastest lap': 'fastest lap', 'date': 'day'}\n",
      "Coverage Score :  0.8571428571428571 \t Completeness Score :  0.8571428571428571\n",
      "\n",
      "204_455.csv(1989 Formula One season)\n",
      "Missing Values(NANs score):  {'rd ': '0/16', 'grand prix': '0/16', 'date': '0/16', 'location': '0/16', 'pole position': '0/16', 'fastest lap': '0/16', 'winning driver': '0/16', 'constructor': '0/16', 'report': '0/16'}\n",
      "Columns that match with input_schema:\n",
      " 204_455.csv :  {'pole position': 'pole position', 'fastest lap': 'fastest lap', 'date': 'day', 'winning driver': 'driver'}\n",
      "Coverage Score :  0.5714285714285714 \t Completeness Score :  0.5714285714285714\n",
      "\n",
      "204_569.csv(1998 Swedish Touring Car Championship season)\n",
      "Missing Values(NANs score):  {'round': '0/12', 'round 1': '0/12', 'circuit': '0/12', 'date': '0/12', 'pole position': '6/12', 'fastest lap': '0/12', 'winning driver': '0/12', 'winning team': '0/12', 'winning privateer': '0/12'}\n",
      "Columns that match with input_schema:\n",
      " 204_569.csv :  {'round': 'round', 'round 1': 'round 1', 'circuit': 'circuit', 'pole position': 'pole position', 'fastest lap': 'fastest lap', 'date': 'day', 'winning driver': 'driver'}\n",
      "Coverage Score :  1.0 \t Completeness Score :  0.9285714285714286\n",
      "\n",
      "204_63.csv(2002 Italian Formula Three season)\n",
      "Missing Values(NANs score):  {'round': '0/9', 'circuit': '0/9', 'date': '0/9', 'pole position': '0/9', 'winning driver': '0/9', 'winning team': '0/9', 'trophy winner': '0/9'}\n",
      "Columns that match with input_schema:\n",
      " 204_63.csv :  {'round': 'round', 'circuit': 'circuit', 'pole position': 'pole position', 'date': 'day', 'winning driver': 'driver'}\n",
      "Coverage Score :  0.7142857142857143 \t Completeness Score :  0.7142857142857143\n",
      "\n",
      "204_845.csv(2003 Barber Dodge Pro Series season)\n",
      "Missing Values(NANs score):  {'round': '0/10', 'circuit': '0/10', 'location': '0/10', 'date': '0/10', 'pole position': '0/10', 'fastest lap': '0/10', 'winning driver': '0/10', 'headline event': '0/10'}\n",
      "Columns that match with input_schema:\n",
      " 204_845.csv :  {'round': 'round', 'circuit': 'circuit', 'pole position': 'pole position', 'fastest lap': 'fastest lap', 'date': 'day', 'winning driver': 'driver'}\n",
      "Coverage Score :  0.8571428571428571 \t Completeness Score :  0.8571428571428571\n",
      "\n",
      "\n",
      "Rank  1   :  204_569.csv          completeness score : 0.9285714285714286 \t\tnumber of rows:  12\n",
      "Rank  2   :  204_40.csv           completeness score : 0.8571428571428571 \t\tnumber of rows:  28\n",
      "Rank  3   :  204_253.csv          completeness score : 0.8571428571428571 \t\tnumber of rows:  26\n",
      "Rank  4   :  204_845.csv          completeness score : 0.8571428571428571 \t\tnumber of rows:  10\n",
      "Rank  5   :  203_742.csv          completeness score : 0.7142857142857143 \t\tnumber of rows:  17\n",
      "Rank  6   :  204_63.csv           completeness score : 0.7142857142857143 \t\tnumber of rows:  9\n",
      "Rank  7   :  203_514.csv          completeness score : 0.6428571428571429 \t\tnumber of rows:  12\n",
      "Rank  8   :  203_181.csv          completeness score : 0.5714285714285714 \t\tnumber of rows:  17\n",
      "Rank  9   :  203_408.csv          completeness score : 0.5714285714285714 \t\tnumber of rows:  16\n",
      "Rank  10  :  204_455.csv          completeness score : 0.5714285714285714 \t\tnumber of rows:  16\n",
      "\n",
      "Ranking Complete!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s='{\"schema\": {\"round\": \"int64\", \"round 1\": \"object\", \"circuit\": \"object\", \"day\": \"object\", \"pole position\": \"object\", \"fastest lap\": \"object\", \"driver\": \"object\"}}'\n",
    "s=s.lower()\n",
    "s=json.loads(s)\n",
    "input_schema=s['schema'];input_categories=-1;transformations=-1;stats=-1\n",
    "if \"categories\" in s:\n",
    "    input_categories=s[\"categories\"]\n",
    "if \"transformations\" in s:\n",
    "    transformations=s[\"transformations\"]\n",
    "if \"stats\" in s:\n",
    "    stats=s[\"stats\"]\n",
    "# display query\n",
    "print(\"Input Schema : \",input_schema)\n",
    "if input_categories==-1:\n",
    "    print(\"Input Categories : None\")\n",
    "else:\n",
    "    print(\"Input Categories : \",input_categories)\n",
    "if transformations==-1:\n",
    "    print(\"Input Transformations : None\")\n",
    "else:\n",
    "    print(\"Input Transformations : \",transformations)\n",
    "if stats==-1:\n",
    "    print(\"Input Condition based conditions : None\")\n",
    "else:\n",
    "    print(\"Input Content based Conditions : \",stats)\n",
    "print()\n",
    "comp_score={}\n",
    "no_of_rows={}\n",
    "no_of_rows={}\n",
    "check_possible_extended_matches()\n",
    "ranking_display(comp_score,no_of_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Schema :  {'round': 'int64', 'round 1': 'object', 'circuit': 'object', 'day': 'object', 'pole position': 'object', 'fastest lap': 'object', 'driver': 'object'}\n",
      "Input Categories :  ['motorsport', 'car', 'seasons']\n",
      "Input Transformations : None\n",
      "Input Condition based conditions : None\n",
      "\n",
      "Matching Tables\n",
      "203_181.csv  :  1990 IndyCar season\n",
      "203_514.csv  :  2008 Superleague Formula season\n",
      "203_742.csv  :  1995 IndyCar season\n",
      "204_253.csv  :  1990 Superbike World Championship season\n",
      "204_569.csv  :  1998 Swedish Touring Car Championship season\n",
      "204_63.csv  :  2002 Italian Formula Three season\n",
      "\n",
      "\n",
      "203_181.csv(1990 IndyCar season)\n",
      "Missing Values(NANs score):  {'date': '0/17', 'rnd': '0/17', 'race name': '0/17', 'circuit': '0/17', 'city/location': '0/17', 'pole position': '0/17', 'winning driver': '0/17', 'winning team': '0/17', 'report': '0/17'}\n",
      "Columns that match with input_schema:\n",
      " 203_181.csv :  {'circuit': 'circuit', 'pole position': 'pole position', 'date': 'day', 'winning driver': 'driver'}\n",
      "Coverage Score :  0.5714285714285714 \t Completeness Score :  0.5714285714285714\n",
      "\n",
      "203_514.csv(2008 Superleague Formula season)\n",
      "Missing Values(NANs score):  {'round': '0/12', 'round 1': '0/12', 'race': '0/12', 'date': '0/12', 'pole position': '6/12', 'fastest lap': '0/12', 'winning club': '0/12', 'winning team': '0/12', 'report': '0/12'}\n",
      "Columns that match with input_schema:\n",
      " 203_514.csv :  {'round': 'round', 'round 1': 'round 1', 'pole position': 'pole position', 'fastest lap': 'fastest lap', 'date': 'day'}\n",
      "Coverage Score :  0.7142857142857143 \t Completeness Score :  0.6428571428571429\n",
      "\n",
      "203_742.csv(1995 IndyCar season)\n",
      "Missing Values(NANs score):  {'rnd': '0/17', 'date': '0/17', 'race name': '0/17', 'circuit': '0/17', 'city/location': '0/17', 'pole position': '0/17', 'fastest lap': '0/17', 'winning driver': '0/17', 'winning team': '0/17', 'report': '0/17'}\n",
      "Columns that match with input_schema:\n",
      " 203_742.csv :  {'circuit': 'circuit', 'pole position': 'pole position', 'fastest lap': 'fastest lap', 'date': 'day', 'winning driver': 'driver'}\n",
      "Coverage Score :  0.7142857142857143 \t Completeness Score :  0.7142857142857143\n",
      "\n",
      "204_253.csv(1990 Superbike World Championship season)\n",
      "Missing Values(NANs score):  {'round': '0/26', 'round 1': '0/26', 'circuit': '0/26', 'date': '0/26', 'pole position': '0/26', 'fastest lap': '0/26', 'winning rider': '0/26'}\n",
      "Columns that match with input_schema:\n",
      " 204_253.csv :  {'round': 'round', 'round 1': 'round 1', 'circuit': 'circuit', 'pole position': 'pole position', 'fastest lap': 'fastest lap', 'date': 'day'}\n",
      "Coverage Score :  0.8571428571428571 \t Completeness Score :  0.8571428571428571\n",
      "\n",
      "204_569.csv(1998 Swedish Touring Car Championship season)\n",
      "Missing Values(NANs score):  {'round': '0/12', 'round 1': '0/12', 'circuit': '0/12', 'date': '0/12', 'pole position': '6/12', 'fastest lap': '0/12', 'winning driver': '0/12', 'winning team': '0/12', 'winning privateer': '0/12'}\n",
      "Columns that match with input_schema:\n",
      " 204_569.csv :  {'round': 'round', 'round 1': 'round 1', 'circuit': 'circuit', 'pole position': 'pole position', 'fastest lap': 'fastest lap', 'date': 'day', 'winning driver': 'driver'}\n",
      "Coverage Score :  1.0 \t Completeness Score :  0.9285714285714286\n",
      "\n",
      "204_63.csv(2002 Italian Formula Three season)\n",
      "Missing Values(NANs score):  {'round': '0/9', 'circuit': '0/9', 'date': '0/9', 'pole position': '0/9', 'winning driver': '0/9', 'winning team': '0/9', 'trophy winner': '0/9'}\n",
      "Columns that match with input_schema:\n",
      " 204_63.csv :  {'round': 'round', 'circuit': 'circuit', 'pole position': 'pole position', 'date': 'day', 'winning driver': 'driver'}\n",
      "Coverage Score :  0.7142857142857143 \t Completeness Score :  0.7142857142857143\n",
      "\n",
      "\n",
      "Rank  1   :  204_569.csv          completeness score : 0.9285714285714286 \t\tnumber of rows:  12\n",
      "Rank  2   :  204_253.csv          completeness score : 0.8571428571428571 \t\tnumber of rows:  26\n",
      "Rank  3   :  203_742.csv          completeness score : 0.7142857142857143 \t\tnumber of rows:  17\n",
      "Rank  4   :  204_63.csv           completeness score : 0.7142857142857143 \t\tnumber of rows:  9\n",
      "Rank  5   :  203_514.csv          completeness score : 0.6428571428571429 \t\tnumber of rows:  12\n",
      "Rank  6   :  203_181.csv          completeness score : 0.5714285714285714 \t\tnumber of rows:  17\n",
      "\n",
      "Ranking Complete!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s='{\"schema\": {\"round\": \"int64\", \"round 1\": \"object\", \"circuit\": \"object\", \"day\": \"object\", \"pole position\": \"object\", \"fastest lap\": \"object\", \"driver\": \"object\"},\"categories\":[\"motorsport\", \"car\", \"seasons\"]}'\n",
    "s=s.lower()\n",
    "s=json.loads(s)\n",
    "input_schema=s['schema'];input_categories=-1;transformations=-1;stats=-1\n",
    "if \"categories\" in s:\n",
    "    input_categories=s[\"categories\"]\n",
    "if \"transformations\" in s:\n",
    "    transformations=s[\"transformations\"]\n",
    "if \"stats\" in s:\n",
    "    stats=s[\"stats\"]\n",
    "# display query\n",
    "print(\"Input Schema : \",input_schema)\n",
    "if input_categories==-1:\n",
    "    print(\"Input Categories : None\")\n",
    "else:\n",
    "    print(\"Input Categories : \",input_categories)\n",
    "if transformations==-1:\n",
    "    print(\"Input Transformations : None\")\n",
    "else:\n",
    "    print(\"Input Transformations : \",transformations)\n",
    "if stats==-1:\n",
    "    print(\"Input Condition based conditions : None\")\n",
    "else:\n",
    "    print(\"Input Content based Conditions : \",stats)\n",
    "print()\n",
    "comp_score={}\n",
    "no_of_rows={}\n",
    "no_of_rows={}\n",
    "check_possible_category_included_matches()\n",
    "ranking_display(comp_score,no_of_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_list(l):\n",
    "    t1=pd.read_csv(l[0])\n",
    "    a=all_schemas[l[0]]\n",
    "    matching_columns,c=generate_matching_columns(a,l[1])\n",
    "    t2=pd.read_csv(l[1])\n",
    "    t2.rename(columns = matching_columns,inplace=True)\n",
    "    try:\n",
    "        res=t1.merge(t2,how='outer')\n",
    "    except:\n",
    "        return -1\n",
    "    for fname in l[2:]:\n",
    "        res_cols={i:j for i,j in zip(res.columns,[str(i) for i in res.dtypes])}\n",
    "        matching_columns,c=generate_matching_columns(res_cols,fname)\n",
    "        t=pd.read_csv(fname)\n",
    "        t.rename(columns = matching_columns,inplace=True)\n",
    "        try:\n",
    "            res=res.merge(t,how='outer')\n",
    "        except:\n",
    "            return -1\n",
    "    res_cols={i:j for i,j in zip(res.columns,[str(i) for i in res.dtypes])}\n",
    "    a=input_schema\n",
    "    cols1={col1:col1 for col1,d_type1 in res_cols.items() if (col1 in a) and d_type1==input_schema[col1]}\n",
    "    a={k:v for k,v in a.items() if k not in cols1}\n",
    "    a_onto=generate_ontology(a)\n",
    "    cols2={col1:col for col1,d_type1 in res_cols.items() for col in a_onto if col1 in a_onto[col] and (d_type1==a[col])}\n",
    "    a={k:v for k,v in a.items() if k not in cols2}\n",
    "    cols={**cols1,**cols2}\n",
    "    res_cols={k:v for k,v in res_cols.items() if k not in list(cols.values())}\n",
    "    for col1,d_type1 in a.items():\n",
    "        x=[col2 for col2,d_type2 in res_cols.items() if ((len(col1)>3 and len(col2)>3) and ((col1 in col2) or (col2 in col1))) and (d_type1==d_type2)]\n",
    "        if len(x)==1:\n",
    "            cols[x[0]]=col1\n",
    "    res.rename(columns = cols,inplace=True)\n",
    "    for col in res.columns:\n",
    "        if col not in input_schema and (res[col].count()<(0.70*len(res))):\n",
    "            res.drop(columns=[col],inplace=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Schema :  {'nationality': 'object', 'tonnage (grt)': 'int64', 'fate': 'object', 'ship': 'object'}\n",
      "Input Categories : None\n",
      "Input Transformations : None\n",
      "Input Condition based conditions : None\n",
      "\n",
      "only schema\n",
      "Matching Tables\n",
      "202_117.csv  :  German submarine U-559\n",
      "203_148.csv  :  German submarine U-9 (1935)\n",
      "203_268.csv  :  German submarine U-502\n",
      "204_100.csv  :  Hans-Rudolf Rosing\n",
      "\n",
      "\n",
      "202_117.csv(German submarine U-559)\n",
      "Missing Values(NANs score):  {'date': '0/6', 'ship': '0/6', 'nationality': '0/6', 'tonnage': '0/6', 'fate': '0/6'}\n",
      "Columns that match with input_schema:\n",
      " 202_117.csv :  {'ship': 'ship', 'nationality': 'nationality', 'fate': 'fate', 'tonnage': 'tonnage (grt)'}\n",
      "Coverage Score :  1.0 \t Completeness Score :  1.0\n",
      "\n",
      "203_148.csv(German submarine U-9 (1935))\n",
      "Missing Values(NANs score):  {'date': '0/9', 'name': '0/9', 'nationality': '0/9', 'tonnage (grt)': '0/9', 'fate': '0/9'}\n",
      "Columns that match with input_schema:\n",
      " 203_148.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "Coverage Score :  0.75 \t Completeness Score :  0.75\n",
      "\n",
      "203_268.csv(German submarine U-502)\n",
      "Missing Values(NANs score):  {'date': '0/16', 'name': '0/16', 'nationality': '0/16', 'tonnage (grt)': '0/16', 'fate': '0/16'}\n",
      "Columns that match with input_schema:\n",
      " 203_268.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "Coverage Score :  0.75 \t Completeness Score :  0.75\n",
      "\n",
      "204_100.csv(Hans-Rudolf Rosing)\n",
      "Missing Values(NANs score):  {'date': '0/13', 'name of ship': '0/13', 'nationality': '0/13', 'tonnage': '0/13', 'fate': '0/13'}\n",
      "Columns that match with input_schema:\n",
      " 204_100.csv :  {'nationality': 'nationality', 'fate': 'fate', 'tonnage': 'tonnage (grt)', 'name of ship': 'ship'}\n",
      "Coverage Score :  1.0 \t Completeness Score :  1.0\n",
      "\n",
      "Matching Tables :  ['202_117.csv', '203_148.csv', '203_268.csv', '204_100.csv']\n",
      "202_117.csv \t German submarine U-559\n",
      "203_148.csv \t German submarine U-9 (1935)\n",
      "203_268.csv \t German submarine U-502\n",
      "204_100.csv \t Hans-Rudolf Rosing\n",
      "\n",
      "1.csv : 202_117.csv\t\t203_148.csv\t\t203_268.csv\t\t\n",
      "Missing Values(NANs):  {'date': '0/31', 'ship': '25/31', 'nationality': '0/31', 'tonnage (grt)': '0/31', 'fate': '0/31', 'name': '6/31'}\n",
      "Columns that match with input_schema: \n",
      "202_117.csv :  {'ship': 'ship', 'nationality': 'nationality', 'fate': 'fate', 'tonnage': 'tonnage (grt)'}\n",
      "203_148.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "203_268.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "Coverage Score :  1.0 \t Completeness Score :  0.7983870967741935\n",
      "\n",
      "2.csv : 202_117.csv\t\t203_148.csv\t\t\n",
      "Missing Values(NANs):  {'date': '0/15', 'ship': '9/15', 'nationality': '0/15', 'tonnage (grt)': '0/15', 'fate': '0/15'}\n",
      "Columns that match with input_schema: \n",
      "202_117.csv :  {'ship': 'ship', 'nationality': 'nationality', 'fate': 'fate', 'tonnage': 'tonnage (grt)'}\n",
      "203_148.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "Coverage Score :  1.0 \t Completeness Score :  0.85\n",
      "\n",
      "3.csv : 202_117.csv\t\t203_268.csv\t\t\n",
      "Missing Values(NANs):  {'date': '0/22', 'ship': '16/22', 'nationality': '0/22', 'tonnage (grt)': '0/22', 'fate': '0/22', 'name': '6/22'}\n",
      "Columns that match with input_schema: \n",
      "202_117.csv :  {'ship': 'ship', 'nationality': 'nationality', 'fate': 'fate', 'tonnage': 'tonnage (grt)'}\n",
      "203_268.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "Coverage Score :  1.0 \t Completeness Score :  0.8181818181818181\n",
      "\n",
      "4.csv : 203_148.csv\t\t203_268.csv\t\t\n",
      "Missing Values(NANs):  {'date': '0/25', 'name': '0/25', 'nationality': '0/25', 'tonnage (grt)': '0/25', 'fate': '0/25'}\n",
      "Columns that match with input_schema: \n",
      "203_148.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "203_268.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "Coverage Score :  0.75 \t Completeness Score :  0.75\n",
      "\n",
      "202_117.csv(German submarine U-559)\n",
      "Missing Values(NANs score):  {'date': '0/6', 'ship': '0/6', 'nationality': '0/6', 'tonnage': '0/6', 'fate': '0/6'}\n",
      "Columns that match with input_schema:\n",
      " 202_117.csv :  {'ship': 'ship', 'nationality': 'nationality', 'fate': 'fate', 'tonnage': 'tonnage (grt)'}\n",
      "Coverage Score :  1.0 \t Completeness Score :  1.0\n",
      "\n",
      "203_148.csv(German submarine U-9 (1935))\n",
      "Missing Values(NANs score):  {'date': '0/9', 'name': '0/9', 'nationality': '0/9', 'tonnage (grt)': '0/9', 'fate': '0/9'}\n",
      "Columns that match with input_schema:\n",
      " 203_148.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "Coverage Score :  0.75 \t Completeness Score :  0.75\n",
      "\n",
      "203_268.csv(German submarine U-502)\n",
      "Missing Values(NANs score):  {'date': '0/16', 'name': '0/16', 'nationality': '0/16', 'tonnage (grt)': '0/16', 'fate': '0/16'}\n",
      "Columns that match with input_schema:\n",
      " 203_268.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "Coverage Score :  0.75 \t Completeness Score :  0.75\n",
      "\n",
      "204_100.csv(Hans-Rudolf Rosing)\n",
      "Missing Values(NANs score):  {'date': '0/13', 'name of ship': '0/13', 'nationality': '0/13', 'tonnage': '0/13', 'fate': '0/13'}\n",
      "Columns that match with input_schema:\n",
      " 204_100.csv :  {'nationality': 'nationality', 'fate': 'fate', 'tonnage': 'tonnage (grt)', 'name of ship': 'ship'}\n",
      "Coverage Score :  1.0 \t Completeness Score :  1.0\n",
      "\n",
      "\n",
      "Rank  1   :  204_100.csv          completeness score : 1.0000000000000000 \t\tnumber of rows:  13\n",
      "Rank  2   :  202_117.csv          completeness score : 1.0000000000000000 \t\tnumber of rows:  6\n",
      "Rank  3   :  output_2.csv         completeness score : 0.8500000000000000 \t\tnumber of rows:  15\n",
      "Rank  4   :  output_3.csv         completeness score : 0.8181818181818181 \t\tnumber of rows:  22\n",
      "Rank  5   :  output_1.csv         completeness score : 0.7983870967741935 \t\tnumber of rows:  31\n",
      "Rank  6   :  output_4.csv         completeness score : 0.7500000000000000 \t\tnumber of rows:  25\n",
      "Rank  7   :  203_268.csv          completeness score : 0.7500000000000000 \t\tnumber of rows:  16\n",
      "Rank  8   :  203_148.csv          completeness score : 0.7500000000000000 \t\tnumber of rows:  9\n",
      "\n",
      "Ranking Complete!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema, transformations and content based conditions\n",
    "s='{\"schema\": {\"nationality\": \"object\", \"tonnage (grt)\": \"int64\", \"fate\": \"object\",\"ship\":\"object\"}}'\n",
    "s=s.lower()\n",
    "s=json.loads(s)\n",
    "input_schema=s[\"schema\"];input_categories=-1;transformations=-1;stats=-1\n",
    "if \"categories\" in s:\n",
    "    input_categories=s[\"categories\"]\n",
    "if \"transformations\" in s:\n",
    "    transformations=s[\"transformations\"]\n",
    "if \"stats\" in s:\n",
    "    stats=s[\"stats\"]\n",
    "# display query\n",
    "print(\"Input Schema : \",input_schema)\n",
    "if input_categories==-1:\n",
    "    print(\"Input Categories : None\")\n",
    "else:\n",
    "    print(\"Input Categories : \",input_categories)\n",
    "if transformations==-1:\n",
    "    print(\"Input Transformations : None\")\n",
    "else:\n",
    "    print(\"Input Transformations : \",transformations)\n",
    "if stats==-1:\n",
    "    print(\"Input Condition based conditions : None\")\n",
    "else:\n",
    "    print(\"Input Content based Conditions : \",stats)\n",
    "print()\n",
    "get_matches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>condition</th>\n",
       "      <th>ownership / access</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Château d'Angers</td>\n",
       "      <td>13th century</td>\n",
       "      <td>Substantially intact</td>\n",
       "      <td>City of Angers</td>\n",
       "      <td>Walls nearly 2,000 feet (610 m) in circumferen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Château de Bauge</td>\n",
       "      <td>15th century</td>\n",
       "      <td>Intact</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Built as hunting lodge.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Château de Bourmont</td>\n",
       "      <td>16-19th century</td>\n",
       "      <td>Intact and extended</td>\n",
       "      <td>Private</td>\n",
       "      <td>Extended in Neo-Gothic style</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Château de Brissac</td>\n",
       "      <td>15-17th century</td>\n",
       "      <td>Rebuilt</td>\n",
       "      <td>Private</td>\n",
       "      <td>Damaged during French Wars of Religion, rebuil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Château de Champtoce</td>\n",
       "      <td>13-16th century</td>\n",
       "      <td>Ruins</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Château de Montreuil-Bellay</td>\n",
       "      <td>11-13th century</td>\n",
       "      <td>Substantially intact</td>\n",
       "      <td>Private</td>\n",
       "      <td>Divided into rental units in 1822, restored af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Château de Montsoreau</td>\n",
       "      <td>1455</td>\n",
       "      <td>Restored</td>\n",
       "      <td>Departement</td>\n",
       "      <td>Ruinous by late 19th century, restored, houses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Château du Plessis-Bourre</td>\n",
       "      <td>1468-1472</td>\n",
       "      <td>Intact</td>\n",
       "      <td>Private (open to the public)</td>\n",
       "      <td>Externally unchanged since the 15th century, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Château du Plessis-Mace</td>\n",
       "      <td>13-16th century</td>\n",
       "      <td>Intact</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Converted to house 15th century.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Château de Pouance</td>\n",
       "      <td>12-13th century</td>\n",
       "      <td>Ruins</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Considered second fortress of Anjou, after Ang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Château de Saumur</td>\n",
       "      <td>12th century</td>\n",
       "      <td>Restored</td>\n",
       "      <td>City of Saumur</td>\n",
       "      <td>Page for September in the Tres Riches Heures d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Château de la Turmeliere</td>\n",
       "      <td>13th century</td>\n",
       "      <td>Ruins</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19th century building of same name nearby.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name             date             condition  \\\n",
       "0              Château d'Angers     13th century  Substantially intact   \n",
       "1              Château de Bauge     15th century                Intact   \n",
       "2           Château de Bourmont  16-19th century   Intact and extended   \n",
       "3            Château de Brissac  15-17th century               Rebuilt   \n",
       "4          Château de Champtoce  13-16th century                 Ruins   \n",
       "5   Château de Montreuil-Bellay  11-13th century  Substantially intact   \n",
       "6         Château de Montsoreau             1455              Restored   \n",
       "7     Château du Plessis-Bourre        1468-1472                Intact   \n",
       "8       Château du Plessis-Mace  13-16th century                Intact   \n",
       "9            Château de Pouance  12-13th century                 Ruins   \n",
       "10            Château de Saumur     12th century              Restored   \n",
       "11     Château de la Turmeliere     13th century                 Ruins   \n",
       "\n",
       "              ownership / access  \\\n",
       "0                 City of Angers   \n",
       "1                            NaN   \n",
       "2                        Private   \n",
       "3                        Private   \n",
       "4                            NaN   \n",
       "5                        Private   \n",
       "6                    Departement   \n",
       "7   Private (open to the public)   \n",
       "8                            NaN   \n",
       "9                            NaN   \n",
       "10                City of Saumur   \n",
       "11                           NaN   \n",
       "\n",
       "                                                notes  \n",
       "0   Walls nearly 2,000 feet (610 m) in circumferen...  \n",
       "1                             Built as hunting lodge.  \n",
       "2                        Extended in Neo-Gothic style  \n",
       "3   Damaged during French Wars of Religion, rebuil...  \n",
       "4                                                 NaN  \n",
       "5   Divided into rental units in 1822, restored af...  \n",
       "6   Ruinous by late 19th century, restored, houses...  \n",
       "7   Externally unchanged since the 15th century, w...  \n",
       "8                    Converted to house 15th century.  \n",
       "9   Considered second fortress of Anjou, after Ang...  \n",
       "10  Page for September in the Tres Riches Heures d...  \n",
       "11         19th century building of same name nearby.  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"204_806.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Schema :  {'nationality': 'object', 'tonnage (grt)': 'int64', 'fate': 'object', 'ship': 'object'}\n",
      "Input Categories : None\n",
      "Input Transformations : None\n",
      "Input Condition based conditions : None\n",
      "\n",
      "only schema\n",
      "Matching Tables\n",
      "202_117.csv  :  German submarine U-559\n",
      "203_148.csv  :  German submarine U-9 (1935)\n",
      "203_268.csv  :  German submarine U-502\n",
      "204_100.csv  :  Hans-Rudolf Rosing\n",
      "\n",
      "\n",
      "202_117.csv(German submarine U-559)\n",
      "Missing Values(NANs score):  {'date': '0/6', 'ship': '0/6', 'nationality': '0/6', 'tonnage': '0/6', 'fate': '0/6'}\n",
      "Columns that match with input_schema:\n",
      " 202_117.csv :  {'ship': 'ship', 'nationality': 'nationality', 'fate': 'fate', 'tonnage': 'tonnage (grt)'}\n",
      "Coverage Score :  1.0 \t Completeness Score :  1.0\n",
      "\n",
      "203_148.csv(German submarine U-9 (1935))\n",
      "Missing Values(NANs score):  {'date': '0/9', 'name': '0/9', 'nationality': '0/9', 'tonnage (grt)': '0/9', 'fate': '0/9'}\n",
      "Columns that match with input_schema:\n",
      " 203_148.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "Coverage Score :  0.75 \t Completeness Score :  0.75\n",
      "\n",
      "203_268.csv(German submarine U-502)\n",
      "Missing Values(NANs score):  {'date': '0/16', 'name': '0/16', 'nationality': '0/16', 'tonnage (grt)': '0/16', 'fate': '0/16'}\n",
      "Columns that match with input_schema:\n",
      " 203_268.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "Coverage Score :  0.75 \t Completeness Score :  0.75\n",
      "\n",
      "204_100.csv(Hans-Rudolf Rosing)\n",
      "Missing Values(NANs score):  {'date': '0/13', 'name of ship': '0/13', 'nationality': '0/13', 'tonnage': '0/13', 'fate': '0/13'}\n",
      "Columns that match with input_schema:\n",
      " 204_100.csv :  {'nationality': 'nationality', 'fate': 'fate', 'tonnage': 'tonnage (grt)', 'name of ship': 'ship'}\n",
      "Coverage Score :  1.0 \t Completeness Score :  1.0\n",
      "\n",
      "Matching Tables :  ['202_117.csv', '203_148.csv', '203_268.csv', '204_100.csv']\n",
      "202_117.csv \t German submarine U-559\n",
      "203_148.csv \t German submarine U-9 (1935)\n",
      "203_268.csv \t German submarine U-502\n",
      "204_100.csv \t Hans-Rudolf Rosing\n",
      "\n",
      "1.csv : 202_117.csv\t\t203_148.csv\t\t203_268.csv\t\t\n",
      "Missing Values(NANs):  {'date': '0/31', 'ship': '0/31', 'nationality': '0/31', 'tonnage (grt)': '0/31', 'fate': '0/31'}\n",
      "Columns that match with input_schema: \n",
      "202_117.csv :  {'ship': 'ship', 'nationality': 'nationality', 'fate': 'fate', 'tonnage': 'tonnage (grt)'}\n",
      "203_148.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "203_268.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "Coverage Score :  1.0 \t Completeness Score :  1.0\n",
      "\n",
      "2.csv : 202_117.csv\t\t203_148.csv\t\t\n",
      "Missing Values(NANs):  {'date': '0/15', 'ship': '0/15', 'nationality': '0/15', 'tonnage (grt)': '0/15', 'fate': '0/15'}\n",
      "Columns that match with input_schema: \n",
      "202_117.csv :  {'ship': 'ship', 'nationality': 'nationality', 'fate': 'fate', 'tonnage': 'tonnage (grt)'}\n",
      "203_148.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "Coverage Score :  1.0 \t Completeness Score :  1.0\n",
      "\n",
      "3.csv : 202_117.csv\t\t203_268.csv\t\t\n",
      "Missing Values(NANs):  {'date': '0/22', 'ship': '0/22', 'nationality': '0/22', 'tonnage (grt)': '0/22', 'fate': '0/22'}\n",
      "Columns that match with input_schema: \n",
      "202_117.csv :  {'ship': 'ship', 'nationality': 'nationality', 'fate': 'fate', 'tonnage': 'tonnage (grt)'}\n",
      "203_268.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "Coverage Score :  1.0 \t Completeness Score :  1.0\n",
      "\n",
      "4.csv : 203_148.csv\t\t203_268.csv\t\t\n",
      "Missing Values(NANs):  {'date': '0/25', 'name': '0/25', 'nationality': '0/25', 'tonnage (grt)': '0/25', 'fate': '0/25'}\n",
      "Columns that match with input_schema: \n",
      "203_148.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "203_268.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "Coverage Score :  0.75 \t Completeness Score :  0.75\n",
      "\n",
      "202_117.csv(German submarine U-559)\n",
      "Missing Values(NANs score):  {'date': '0/6', 'ship': '0/6', 'nationality': '0/6', 'tonnage': '0/6', 'fate': '0/6'}\n",
      "Columns that match with input_schema:\n",
      " 202_117.csv :  {'ship': 'ship', 'nationality': 'nationality', 'fate': 'fate', 'tonnage': 'tonnage (grt)'}\n",
      "Coverage Score :  1.0 \t Completeness Score :  1.0\n",
      "\n",
      "203_148.csv(German submarine U-9 (1935))\n",
      "Missing Values(NANs score):  {'date': '0/9', 'name': '0/9', 'nationality': '0/9', 'tonnage (grt)': '0/9', 'fate': '0/9'}\n",
      "Columns that match with input_schema:\n",
      " 203_148.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "Coverage Score :  0.75 \t Completeness Score :  0.75\n",
      "\n",
      "203_268.csv(German submarine U-502)\n",
      "Missing Values(NANs score):  {'date': '0/16', 'name': '0/16', 'nationality': '0/16', 'tonnage (grt)': '0/16', 'fate': '0/16'}\n",
      "Columns that match with input_schema:\n",
      " 203_268.csv :  {'nationality': 'nationality', 'tonnage (grt)': 'tonnage (grt)', 'fate': 'fate'}\n",
      "Coverage Score :  0.75 \t Completeness Score :  0.75\n",
      "\n",
      "204_100.csv(Hans-Rudolf Rosing)\n",
      "Missing Values(NANs score):  {'date': '0/13', 'name of ship': '0/13', 'nationality': '0/13', 'tonnage': '0/13', 'fate': '0/13'}\n",
      "Columns that match with input_schema:\n",
      " 204_100.csv :  {'nationality': 'nationality', 'fate': 'fate', 'tonnage': 'tonnage (grt)', 'name of ship': 'ship'}\n",
      "Coverage Score :  1.0 \t Completeness Score :  1.0\n",
      "\n",
      "\n",
      "Rank  1   :  output_1.csv         completeness score : 1.0000000000000000 \t\tnumber of rows:  31\n",
      "Rank  2   :  output_3.csv         completeness score : 1.0000000000000000 \t\tnumber of rows:  22\n",
      "Rank  3   :  output_2.csv         completeness score : 1.0000000000000000 \t\tnumber of rows:  15\n",
      "Rank  4   :  204_100.csv          completeness score : 1.0000000000000000 \t\tnumber of rows:  13\n",
      "Rank  5   :  202_117.csv          completeness score : 1.0000000000000000 \t\tnumber of rows:  6\n",
      "Rank  6   :  output_4.csv         completeness score : 0.7500000000000000 \t\tnumber of rows:  25\n",
      "Rank  7   :  203_268.csv          completeness score : 0.7500000000000000 \t\tnumber of rows:  16\n",
      "Rank  8   :  203_148.csv          completeness score : 0.7500000000000000 \t\tnumber of rows:  9\n",
      "\n",
      "Ranking Complete!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema, transformations and content based conditions\n",
    "s='{\"schema\": {\"nationality\": \"object\", \"tonnage (grt)\": \"int64\", \"fate\": \"object\",\"ship\":\"object\"}}'\n",
    "s=s.lower()\n",
    "s=json.loads(s)\n",
    "input_schema=s[\"schema\"];input_categories=-1;transformations=-1;stats=-1\n",
    "if \"categories\" in s:\n",
    "    input_categories=s[\"categories\"]\n",
    "if \"transformations\" in s:\n",
    "    transformations=s[\"transformations\"]\n",
    "if \"stats\" in s:\n",
    "    stats=s[\"stats\"]\n",
    "# display query\n",
    "print(\"Input Schema : \",input_schema)\n",
    "if input_categories==-1:\n",
    "    print(\"Input Categories : None\")\n",
    "else:\n",
    "    print(\"Input Categories : \",input_categories)\n",
    "if transformations==-1:\n",
    "    print(\"Input Transformations : None\")\n",
    "else:\n",
    "    print(\"Input Transformations : \",transformations)\n",
    "if stats==-1:\n",
    "    print(\"Input Condition based conditions : None\")\n",
    "else:\n",
    "    print(\"Input Content based Conditions : \",stats)\n",
    "print()\n",
    "get_matches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
