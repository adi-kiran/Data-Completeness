{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from nltk.corpus import wordnet\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preloading all schemas and categories\n",
    "all_schemas={}\n",
    "all_categories={}\n",
    "with open(\"final_schema.txt\") as ip_file:\n",
    "    for line in ip_file.readlines():\n",
    "        json_obj=json.loads(line)\n",
    "        all_schemas[json_obj[\"filename\"]]=json_obj[\"schema\"]\n",
    "        all_categories[json_obj[\"filename\"]]=json_obj[\"categories\"]\n",
    "        \n",
    "#preloading the candidate keys\n",
    "with open(\"Candidate_key_dict.txt\",'r') as ip_file:\n",
    "    cand_key=json.load(ip_file)\n",
    "\n",
    "#preloading column and category similarity values of tables\n",
    "with open(\"cos_similarity.txt\",'r') as ip_file:\n",
    "    json_object=json.load(ip_file)\n",
    "col_sim = json_object[\"column_similarity\"]\n",
    "cat_sim = json_object[\"category_similarity\"]\n",
    "\n",
    "#making a list of transformation functions\n",
    "transform_funct_list=['average','sum','maximum','minimum','range','median','variance','standard deviation','mode','frequency']\n",
    "tf_onto=generate_list_ontology(transform_funct_list)\n",
    "tf_onto[\"average\"].append(\"avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all functions needed to generate ontologies\n",
    "def get_synonyms(word):\n",
    "    meanings=set()\n",
    "    for synset in wordnet.synsets(word,pos=wordnet.NOUN):\n",
    "        for lemma in synset.lemmas():\n",
    "            meanings.add(lemma.name())\n",
    "    for synset in wordnet.synsets(word,pos=wordnet.NOUN):\n",
    "        for hypernym in synset.hypernyms():\n",
    "            meanings.add(hypernym.lemma_names()[0])\n",
    "    meanings.add(word)\n",
    "    return list(meanings)\n",
    "\n",
    "# takes input list and returns ontology as dictionary with every word in list as the key\n",
    "def generate_list_ontology(list1):\n",
    "    ontology={}\n",
    "    for word in list1:\n",
    "        ontology[word]=get_synonyms(word)\n",
    "    return ontology\n",
    "\n",
    "# takes the input as a schema and returns ontology for every column in the schema\n",
    "def generate_schema_ontology(input_schema):\n",
    "    ontology={}\n",
    "    for col in input_schema:\n",
    "        ontology[col]=get_synonyms(col)\n",
    "    return ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate cos similarity between two lists\n",
    "def cos_sim(list1, list2):\n",
    "    terms = set(list1).union(list2)\n",
    "    intersect = set(list1) & set(list2)\n",
    "    others = (set(list1)-intersect).union(set(list2)-intersect)\n",
    "    product=0\n",
    "    for word in terms:\n",
    "        if word in intersect:\n",
    "            product+=1\n",
    "    l1mag = math.sqrt(len(list1))\n",
    "    l2mag = math.sqrt(len(list2))\n",
    "    if len(list1)==0 or len(list2)==0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return product / (l1mag * l2mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both functions return all posssible tables with one or more columns that match with input schema(or its ontology)\n",
    "\n",
    "# if input has only schema(columns and their dataypes)\n",
    "def col_only_list(input_schema,input_sch_onto):\n",
    "    possible_tables={}\n",
    "    for file in all_schemas:\n",
    "        schema=all_schemas[file]\n",
    "        for col,d_type in schema.items():\n",
    "            if (col in input_schema) and (input_schema[col]==d_type):\n",
    "                if file in possible_tables:\n",
    "                    possible_tables[file].append(col)\n",
    "                else:\n",
    "                    possible_tables[file]=[]\n",
    "                    possible_tables[file].append(col)\n",
    "            else:\n",
    "                for a in input_sch_onto:\n",
    "                    if (col in input_sch_onto[a]) and (input_schema[a]==d_type):\n",
    "                        if file in possible_tables:\n",
    "                            possible_tables[file].append(a)\n",
    "                        else:\n",
    "                            possible_tables[file]=[]\n",
    "                            possible_tables[file].append(a)\n",
    "    return possible_tables\n",
    "\n",
    "# if input has categories as well as schema(columns and their dataypes)\n",
    "# we consider it a match under the assumption that at least 75% category match exists\n",
    "def cat_and_col_list(input_categories,input_cat_onto,input_schema,input_sch_onto):\n",
    "    possible_tables={}\n",
    "    for file in all_categories:\n",
    "        cat_list=[]\n",
    "        category=all_categories[file]\n",
    "        for cat in category:\n",
    "            if cat in input_categories:\n",
    "                cat_list.append(cat)\n",
    "            else:\n",
    "                for cat1 in input_cat_onto:\n",
    "                    if cat in input_cat_onto[cat1]:\n",
    "                        cat_list.append(cat1)\n",
    "        cos_val=cos_sim(cat_list,input_categories)\n",
    "        if cos_val > 0.75 :\n",
    "            schema=all_schemas[file]\n",
    "            for col,d_type in schema.items():\n",
    "                if (col in input_schema) and (input_schema[col]==d_type):\n",
    "                    if file in possible_tables:\n",
    "                        possible_tables[file].append(col)\n",
    "                    else:\n",
    "                        possible_tables[file]=[]\n",
    "                        possible_tables[file].append(col)\n",
    "                else:\n",
    "                    for a in input_sch_onto:\n",
    "                        if (col in input_sch_onto[a]) and (input_schema[a]==d_type):\n",
    "                            if file in possible_tables:\n",
    "                                possible_tables[file].append(a)\n",
    "                            else:\n",
    "                                possible_tables[file]=[]\n",
    "                                possible_tables[file].append(a)\n",
    "    return possible_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to merge two tables\n",
    "# it generates all matching columns btween two input filenames and calls merge function to merge the tables\n",
    "def merge(fname1,fname2):\n",
    "    matching_columns = {}  #resultant list containing the matching columns.\n",
    "    #getting schema for tables.\n",
    "    f1 = all_schemas[fname1]\n",
    "    f2 = all_schemas[fname2]\n",
    "    #Retrieving the column names and generating the ontologies for one of the tables columns\n",
    "    f1_cols = list(f1)\n",
    "    f1_onto = generate_list_ontology(f1_cols)\n",
    "    #finding columns that match.     \n",
    "    for col , d_type in f2.items():\n",
    "        if (col in f1_cols) and (d_type==f1[col]):\n",
    "            matching_columns[col] = col\n",
    "        else:\n",
    "            for col_t1 in f1_onto:\n",
    "                if (col in f1_onto[col_t1]) and (d_type==f1[col_t1]):\n",
    "                    matching_columns[col_t1] = col\n",
    "                    break\n",
    "    #Now the varaible matching_columns contains a list of names of columns that match between the two tables.\n",
    "    t1 = cand_key[fname1]\n",
    "    t2 = cand_key[fname2]\n",
    "    mat_cols={}\n",
    "    for key , value in matching_columns.items():\n",
    "        if(key in t1 or value in t2):\n",
    "            mat_cols[key]=value\n",
    "    return merge_tables(fname1,fname2,mat_cols)\n",
    "\n",
    "# it is called by merge and does the actual merging\n",
    "def merge_tables(fname1,fname2,cols):\n",
    "    t1 = pd.read_csv(fname1)\n",
    "    t2 = pd.read_csv(fname2)\n",
    "    print(fname1+' :')\n",
    "    display(t1)\n",
    "    print(fname2+' :')\n",
    "    display(t2)\n",
    "    print(fname1+' and '+fname2+' gives : ')\n",
    "    l=len(cols)\n",
    "    if l!=0:\n",
    "        for name1,name2 in cols.items():\n",
    "            t2.rename(columns = {name2:name1},inplace=True)\n",
    "    t3=t1.merge(t2,how='outer')\n",
    "    display(t3)\n",
    "    return t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function generates all possible combinations of list l taking elements n to 2 at a time and returns a dictionary\n",
    "def generate_all_combinations(l):\n",
    "    x={}\n",
    "    a=len(l)\n",
    "    for i in range(a,1,-1):\n",
    "        x[i]=list(combinations(l,i))\n",
    "    return x\n",
    "\n",
    "# function generates all matching columns between the res_cols schema and columns of table in fname\n",
    "def generate_matching_columns(res_cols,fname):\n",
    "    a=all_schemas[fname]\n",
    "    matching_columns={}\n",
    "    res_onto=generate_schema_ontology(res_cols)\n",
    "    for col , d_type in a.items():\n",
    "        if (col in res_cols) and (d_type==res_cols[col]):\n",
    "            matching_columns[col] = col\n",
    "        else:\n",
    "            for col_res in res_onto:\n",
    "                if (col in res_onto[col_res]) and (d_type==res_cols[col_res]):\n",
    "                    matching_columns[col] = col_res\n",
    "                    break\n",
    "    return matching_columns\n",
    "\n",
    "# returns a merged table of all tables given in input list l\n",
    "def merge_list(l):\n",
    "    t1=pd.read_csv(l[0])\n",
    "    t2=pd.read_csv(l[1])\n",
    "    a=all_schemas[l[0]]\n",
    "    b=all_schemas[l[1]]\n",
    "    matching_columns={}\n",
    "    a_onto=generate_schema_ontology(a)\n",
    "    for col , d_type in b.items():\n",
    "        if (col in a) and (d_type==a[col]):\n",
    "            matching_columns[col] = col\n",
    "        else:\n",
    "            for col_a in a_onto:\n",
    "                if (col in a_onto[col_a]) and (d_type==a[col_a]):\n",
    "                    matching_columns[col] = col_a\n",
    "                    break\n",
    "    t2.rename(columns = matching_columns,inplace=True)\n",
    "    res=t1.merge(t2,how='outer')\n",
    "    for fname in l[2:]:\n",
    "        d_types=[]\n",
    "        res_cols={}\n",
    "        for i in res.dtypes:\n",
    "            d_types.append(str(i))\n",
    "        for i,j in zip(res.columns,d_types):\n",
    "            res_cols[i]=j\n",
    "        matching_columns=generate_matching_columns(res_cols,fname)\n",
    "        t=pd.read_csv(fname)\n",
    "        t.rename(columns = matching_columns,inplace=True)\n",
    "        res=res.merge(t,how='outer')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives the nan score(no on nans/no of entries in table) for each column in the input table\n",
    "def nan_score_table(table):\n",
    "    nan_count={}\n",
    "    a=len(table)\n",
    "    for i in table.columns:\n",
    "        x=a-table[i].count()\n",
    "        s=str(x)+'/'+str(a)\n",
    "        nan_count[i]=s\n",
    "    return nan_count\n",
    "\n",
    "# gives the nan score(no on nans/no of entries in table) for each column in the table in the input parameter fname\n",
    "def nan_score_fname(fname):\n",
    "    table=pd.read_csv(fname)\n",
    "    nan_count={}\n",
    "    a=len(table)\n",
    "    for i in table.columns:\n",
    "        x=a-table[i].count()\n",
    "        s=str(x)+'/'+str(a)\n",
    "        nan_count[i]=s\n",
    "    return nan_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function to print the individual tables, their nan score as well as match column with input schema\n",
    "def display_individual_matches(matching_tables,matching_tables_dict,transform_cols={}):\n",
    "    if len(transform_cols)==0:\n",
    "        flag=0\n",
    "    else:\n",
    "        flag=1\n",
    "    with open(\"output_folder_transformation/results.txt\",'a') as f:\n",
    "        for i in matching_tables:\n",
    "            print(i,file=f)\n",
    "            if flag==1:\n",
    "                l={}\n",
    "                cols=pd.read_csv(i).columns\n",
    "                for col,tran in transform_cols.items():\n",
    "                    if col in cols:\n",
    "                        l[col]=tran\n",
    "                print(\"possible transformations are : \",l,file=f)\n",
    "            print('Missing Values(NANs score): ',nan_score_fname(i),file=f)\n",
    "            print(\"Columns that match with input_schema:\\n \"+i+' : ',matching_tables_dict[i],file=f)\n",
    "#             print('Column Match Score: '+str(len(matching_tables_dict[i]))+'/'+str(len(input_schema)))\n",
    "            print(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_possible_matches():\n",
    "    with open(\"input_tranformations.txt\",'r') as ip_file:\n",
    "        json_object=json.load(ip_file)\n",
    "    input_schema=json_object[\"schema\"]\n",
    "    input_sch_onto=generate_schema_ontology(input_schema)\n",
    "    if \"categories\" in json_object:\n",
    "        print('category and schema')\n",
    "        input_categories=json_object[\"categories\"]\n",
    "        input_cat_onto=generate_list_ontology(input_categories)\n",
    "        possible_tables=cat_and_col_list(input_categories,input_cat_onto,input_schema,input_sch_onto)\n",
    "    else:\n",
    "        print('only schema')\n",
    "        possible_tables=col_only_list(input_schema,input_sch_onto)\n",
    "    matching_tables={}\n",
    "    for i in possible_tables:\n",
    "        cos_val=cos_sim(possible_tables[i],list(input_schema))\n",
    "        if cos_val>0.75:\n",
    "            matching_tables[i]=possible_tables[i]\n",
    "    return matching_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main function to be called to generate all direct matches with the schema present in input.txt\n",
    "# this calls all our other functions and generates all possible direct matches(including by merging tables) with the input schema\n",
    "# direct matches imply transformations are not considered\n",
    "def get_matches():\n",
    "    with open(\"output_folder_transformation/results.txt\",'a') as f:\n",
    "        print('------------------------------',file=f)\n",
    "        print(file=f)\n",
    "        print(\"Direct Match\",file=f)\n",
    "        print(file=f)\n",
    "    matching_tables_dict=check_possible_matches()\n",
    "    matching_tables=list(matching_tables_dict)\n",
    "    op_str1='output_folder_transformation/'\n",
    "    op_str2='.csv'\n",
    "    x=len(matching_tables)\n",
    "    print(matching_tables)\n",
    "    if x==0:\n",
    "        with open(\"output_folder_transformation/results.txt\",'a') as f:\n",
    "            print(\"NO MATCHES FOUND\",file=f)\n",
    "            print(file=f)\n",
    "    elif x==1:\n",
    "        display_individual_matches(matching_tables,matching_tables_dict)\n",
    "    elif x==2:\n",
    "        a=matching_tables[0]+' : '+matching_tables[1]\n",
    "        b=matching_tables[1]+' : '+matching_tables[0]\n",
    "        if (a in cat_sim) or (b in cat_sim):\n",
    "            if (cat_sim[a]>.50 and col_sim[a]>0.50) or (cat_sim[b]>.50 and col_sim[b]>0.50):\n",
    "                res=merge(matching_tables[0],matching_tables[1])\n",
    "                op_string=op_str1+'1t'+op_str2\n",
    "                res.to_csv(op_string,sep=',', index=False)\n",
    "                display_individual_matches(matching_tables,matching_tables_dict)\n",
    "    else:\n",
    "        ctr=0\n",
    "        count_comb=len(matching_tables)\n",
    "        all_combos=generate_all_combinations(matching_tables)\n",
    "        for i in range(count_comb,1,-1):\n",
    "            for l in all_combos[i]:\n",
    "                a=list(l)\n",
    "                comb=list(combinations(a,2))\n",
    "                flag=0\n",
    "                for pair in comb:\n",
    "                    if flag==0:\n",
    "                        t1,t2=pair\n",
    "                        if ((t1+' : '+t2) in cat_sim):\n",
    "                            if (cat_sim[t1+' : '+t2]>=.50 and col_sim[t1+' : '+t2]>=0.50):\n",
    "                                pass\n",
    "                            else:\n",
    "                                flag=1\n",
    "                        else:\n",
    "                            flag=1\n",
    "                    else:\n",
    "                        break\n",
    "                if flag==0:\n",
    "                    res=merge_list(a)\n",
    "                    ctr+=1\n",
    "                    op_string=op_str1+str(ctr)+op_str2\n",
    "                    res.to_csv(op_string,sep=',', index=False)\n",
    "                    with open(\"output_folder_transformation/results.txt\",'a') as f:\n",
    "                        print(str(ctr)+op_str2+' : ',end='',file=f)\n",
    "                        for j in a:\n",
    "                            print(j+'\\t\\t',end='',file=f)\n",
    "                        print(file=f)\n",
    "                        print('Missing Values(NANs): ',nan_score_table(res),file=f)\n",
    "                        print(\"Columns that match with input_schema: \",file=f)\n",
    "                        for j in a:\n",
    "                            print(j+' : ',matching_tables_dict[j],file=f)\n",
    "                        print(file=f)\n",
    "        display_individual_matches(matching_tables,matching_tables_dict)\n",
    "    with open(\"output_folder_transformation/results.txt\",'a') as f:\n",
    "        print('------------------------------',file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_possible_matches_transform():\n",
    "    with open(\"input_tranformations.txt\",'r') as ip_file:\n",
    "        ip_schema=json.load(ip_file)\n",
    "        ip_schema=ip_schema[\"schema\"]\n",
    "    cols={}\n",
    "    transform={}\n",
    "    for i,d_type in ip_schema.items():\n",
    "        x=i.split()\n",
    "        if len(x)==1:\n",
    "            cols[i]=d_type\n",
    "        else:\n",
    "            for a in x:\n",
    "                a1=a.lower()\n",
    "                if a1 in transform_funct_list:\n",
    "                    x.remove(a)\n",
    "                    str1=x[0]\n",
    "                    for w in x[1:]:\n",
    "                        str1+=\" \"+w\n",
    "                    cols[str1]=d_type\n",
    "                    transform[str]=a1\n",
    "                else:\n",
    "                    for c in tf_onto:\n",
    "                        if a1 in tf_onto[c]:\n",
    "                            x.remove(a)\n",
    "                            str1=x[0]\n",
    "                            for w in x[1:]:\n",
    "                                str1+=\" \"+w\n",
    "                            cols[str1]=d_type\n",
    "                            transform[str1]=a1\n",
    "    input_schema=cols\n",
    "    input_sch_onto=generate_schema_ontology(input_schema)\n",
    "    if \"categories\" in json_object:\n",
    "        print('category and schema')\n",
    "        input_categories=json_object[\"categories\"]\n",
    "        input_cat_onto=generate_list_ontology(input_categories)\n",
    "        possible_tables=cat_and_col_list(input_categories,input_cat_onto,input_schema,input_sch_onto)\n",
    "    else:\n",
    "        print('only schema')\n",
    "        possible_tables=col_only_list(input_schema,input_sch_onto)\n",
    "    matching_tables={}\n",
    "    for i in possible_tables:\n",
    "        cos_val=cos_sim(possible_tables[i],list(input_schema))\n",
    "        if cos_val>0.75:\n",
    "            matching_tables[i]=possible_tables[i]\n",
    "    return (matching_tables,transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform_matches():\n",
    "    (matching_tables_dict,transform_cols)=check_possible_matches_transform()\n",
    "    matching_tables=list(matching_tables_dict)\n",
    "    with open(\"output_folder_transformation/results.txt\",'a') as f:\n",
    "        print('******************************',file=f)\n",
    "        print(file=f)\n",
    "        print(\"Transformation Match\",file=f)\n",
    "        print(file=f)\n",
    "        if len(transform_cols)==0:\n",
    "            print(\"No Transformations In Input Schema\",file=f)\n",
    "            print(file=f)\n",
    "            return\n",
    "        else:\n",
    "            print('Transformations detected are : ',transform_cols,file=f)\n",
    "            print(file=f)\n",
    "    op_str1='output_folder_transformation/'\n",
    "    op_str2='t.csv'\n",
    "    x=len(matching_tables)\n",
    "    print(matching_tables)\n",
    "    if x==0:\n",
    "        with open(\"output_folder_transformation/results.txt\",'a') as f:\n",
    "            print(\"NO MATCHES FOUND\",file=f)\n",
    "            print(file=f)\n",
    "    elif x==1:\n",
    "        display_individual_matches(matching_tables,matching_tables_dict)\n",
    "    elif x==2:\n",
    "        a=matching_tables[0]+' : '+matching_tables[1]\n",
    "        b=matching_tables[1]+' : '+matching_tables[0]\n",
    "        if (a in cat_sim) or (b in cat_sim):\n",
    "            if (cat_sim[a]>.50 and col_sim[a]>0.50) or (cat_sim[b]>.50 and col_sim[b]>0.50):\n",
    "                res=merge(matching_tables[0],matching_tables[1])\n",
    "                op_string=op_str1+'1t'+op_str2\n",
    "                res.to_csv(op_string,sep=',', index=False)\n",
    "                display_individual_matches(matching_tables,matching_tables_dict)\n",
    "    else:\n",
    "        ctr=0\n",
    "        count_comb=len(matching_tables)\n",
    "        all_combos=generate_all_combinations(matching_tables)\n",
    "        for i in range(count_comb,1,-1):\n",
    "            for l in all_combos[i]:\n",
    "                a=list(l)\n",
    "                comb=list(combinations(a,2))\n",
    "                flag=0\n",
    "                for pair in comb:\n",
    "                    if flag==0:\n",
    "                        t1,t2=pair\n",
    "                        if ((t1+' : '+t2) in cat_sim):\n",
    "                            if (cat_sim[t1+' : '+t2]>=.50 and col_sim[t1+' : '+t2]>=0.50):\n",
    "                                pass\n",
    "                            else:\n",
    "                                flag=1\n",
    "                        else:\n",
    "                            flag=1\n",
    "                    else:\n",
    "                        break\n",
    "                if flag==0:\n",
    "                    res=merge_list(a)\n",
    "                    ctr+=1\n",
    "                    op_string=op_str1+str(ctr)+op_str2\n",
    "                    res.to_csv(op_string,sep=',', index=False)\n",
    "                    l={}\n",
    "                    cols=res.columns\n",
    "                    for col,tran in transform_cols.items():\n",
    "                        if col in cols:\n",
    "                            l[col]=tran\n",
    "                    with open(\"output_folder_transformation/results.txt\",'a') as f:\n",
    "                        print(str(ctr)+op_str2+' : ',end='',file=f)\n",
    "                        for j in a:\n",
    "                            print(j+'\\t\\t',end='',file=f)\n",
    "                        print(file=f)\n",
    "                        print(\"possible transformations are : \",l,file=f)\n",
    "                        print('Missing Values(NANs): ',nan_score_table(res),file=f)\n",
    "                        print(\"Columns that match with input_schema: \",file=f)\n",
    "                        for j in a:\n",
    "                            print(j+' : ',matching_tables_dict[j],file=f)\n",
    "                        print(file=f)\n",
    "        display_individual_matches(matching_tables,matching_tables_dict,transform_cols=transform_cols)\n",
    "    with open(\"output_folder_transformation/results.txt\",'a') as f:\n",
    "        print('******************************',file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only schema\n",
      "['202_117.csv', '203_148.csv', '203_268.csv', '204_100.csv']\n"
     ]
    }
   ],
   "source": [
    "get_matches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "only schema\n",
      "['202_117.csv', '203_148.csv', '203_268.csv', '204_100.csv']\n"
     ]
    }
   ],
   "source": [
    "get_transform_matches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
