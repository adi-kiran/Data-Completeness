{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synonyms(word):\n",
    "    meanings=set()\n",
    "    for synset in wordnet.synsets(word,pos=wordnet.NOUN):\n",
    "        for lemma in synset.lemmas():\n",
    "            meanings.add(lemma.name())\n",
    "    for synset in wordnet.synsets(word,pos=wordnet.NOUN):\n",
    "        for hypernym in synset.hypernyms():\n",
    "            meanings.add(hypernym.lemma_names()[0])\n",
    "    meanings.add(word)\n",
    "    return list(meanings)\n",
    "\n",
    "def generate_list_ontology(list1):\n",
    "    ontology={}\n",
    "    for word in list1:\n",
    "        ontology[word]=get_synonyms(word)\n",
    "    return ontology\n",
    "\n",
    "def generate_schema_ontology(input_schema):\n",
    "    ontology={}\n",
    "    for col in input_schema:\n",
    "        ontology[col]=get_synonyms(col)\n",
    "    return ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(list1, list2):\n",
    "    terms = set(list1).union(list2)\n",
    "    intersect = set(list1) & set(list2)\n",
    "    others = (set(list1)-intersect).union(set(list2)-intersect)\n",
    "    product=0\n",
    "    for word in terms:\n",
    "        if word in intersect:\n",
    "            product+=1\n",
    "    l1mag = math.sqrt(len(list1))\n",
    "    l2mag = math.sqrt(len(list2))\n",
    "    if len(list1)==0 or len(list2)==0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return product / (l1mag * l2mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the given input file names , find the columns that match if any.\n",
    "#If no colmns match , then don't do anything.\n",
    "#If columns match , then check if the columns are present in the list of candidate keys.\n",
    "#If not present in list of candidate keys , just check data.\n",
    "#If present in list of candidate keys and data of those columns match , append the others columns to any of the table to extend the table schema.\n",
    "#If present in list of candidate keys but data dont match , then increse volume by adding the rows to one of the table , but will create a lot of NULL values. Undesirable.\n",
    "#If all the columns match , then to each table just add another column with the table name and append the two tables.\n",
    "\n",
    "\n",
    "def merge(fname1,fname2):\n",
    "    matching_columns = {}#resultant list containing the matching columns.\n",
    "    \n",
    "    #reading the CSV files.\n",
    "    f1 = pd.read_csv(fname1 , 'r')\n",
    "    f2 = pd.read_csv(fname2 , 'r')\n",
    "    \n",
    "    #Retrieving the column names from the tables.\n",
    "    f1_cols = list(f1.columns.values)\n",
    "    f2_cols = list(f2.columns.values)\n",
    "    \n",
    "    #generating the ontologies for one of the tables columns and match with the other tables columns.\n",
    "    f1_cols_onto_dict = generate_list_ontology(f1_cols)\n",
    "    for key , value in f1_cols_onto_dict.items():\n",
    "        if(key in f2_cols):\n",
    "            matching_columns[key] = key\n",
    "        else:\n",
    "            for word in value:\n",
    "                if(word in f2_cols):\n",
    "                    matching_columns[key] = word\n",
    "                    break\n",
    "\n",
    "    #Now the varaible matching_columns contains a list of names of columns that match between the two tables.\n",
    "    print(matching_columns)\n",
    "    \n",
    "    with open(\"Candidate_key_dict.txt\" , 'r') as ip_file:\n",
    "        cand_key = json.load(ip_file)\n",
    "    t1 = cand_key[fname1]\n",
    "    t2 = cand_key[fname2]\n",
    "    \n",
    "    for key , value in matching_columns.items():\n",
    "        if(key in t1 or value in t2):\n",
    "            do_merge(fname1 , key , fname2 , val)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_only_list(input_schema,input_sch_onto):\n",
    "    possible_tables=[]\n",
    "    f = open(\"final_schema.txt\",'r')\n",
    "    for line in f.readlines():\n",
    "        flag=0\n",
    "        json_object=json.loads(line)\n",
    "        filename=json_object[\"filename\"]\n",
    "        schema=json_object[\"schema\"]\n",
    "        for col,d_type in schema.items():\n",
    "            if (col in input_schema) and (input_schema[col]==d_type):\n",
    "                possible_tables.append(filename)\n",
    "                flag=1\n",
    "                break\n",
    "            else:\n",
    "                for a in input_sch_onto:\n",
    "                    if (col in input_sch_onto[a]) and (input_schema[a]==d_type):\n",
    "                        possible_tables.append(filename)\n",
    "                        flag=1\n",
    "                        break\n",
    "        if flag==1:\n",
    "            possible_tables.append(filename)\n",
    "    f.close()\n",
    "    return possible_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_and_col_list(input_categories,input_schema):\n",
    "    possible_tables=[]\n",
    "    f = open(\"final_schema.txt\",'r')\n",
    "    for line in f.readlines():\n",
    "        flag=0\n",
    "        json_object=json.loads(line)\n",
    "        filename=json_object[\"filename\"]\n",
    "        schema=json_object[\"schema\"]\n",
    "        category=json_object[\"categories\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_schemas={}\n",
    "with open(\"final_schema.txt\") as ip_file:\n",
    "    for line in ip_file.readlines():\n",
    "        json_obj=json.loads(line)\n",
    "        all_schemas[json_obj[\"filename\"]]=json_obj[\"schema\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2027"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tables(fname1,fname2,cols):\n",
    "    t1 = pd.read_csv(fname1)\n",
    "    t2 = pd.read_csv(fname2)\n",
    "    l=len(cols)\n",
    "    if l!=0:\n",
    "        for name1,name2 in cols.items():\n",
    "            t2.rename(columns = {name2:name1},inplace=True)\n",
    "    t3=t1.merge(t2,how='outer')\n",
    "    print(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions=[\"sum\",\"product\",\"power\",\"square root\",\"quotient\",\"mod\",\"subtotal\",\"ceiling\",\"floor\",\"int\",\"trunc\",\"count\",\"countif\",\"counta\",\"max\",\"min\",\"average\",\"averagea\",\"median\",\"mode\",\"stdev\",\"var\",\"frequency\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-3-0f37212b7da2>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-0f37212b7da2>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def total(df,col):\n",
    "    \n",
    "    \n",
    "def product(df,col):\n",
    "    \n",
    "    \n",
    "def power(df,col):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
