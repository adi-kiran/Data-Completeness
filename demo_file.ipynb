{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from nltk.corpus import wordnet\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preloading all schemas and categories\n",
    "all_schemas={}\n",
    "all_categories={}\n",
    "all_tablenames={}\n",
    "with open(\"final_schema.txt\") as ip_file:\n",
    "    for line in ip_file.readlines():\n",
    "        json_obj=json.loads(line)\n",
    "        all_schemas[json_obj[\"filename\"]]=json_obj[\"schema\"]\n",
    "        all_categories[json_obj[\"filename\"]]=json_obj[\"categories\"]\n",
    "        all_tablenames[json_obj[\"filename\"]]=json_obj[\"tablename\"]\n",
    "        \n",
    "#preloading the candidate keys\n",
    "with open(\"Candidate_key_dict.txt\",'r') as ip_file:\n",
    "    cand_key=json.load(ip_file)\n",
    "\n",
    "#preloading column and category similarity values of tables\n",
    "with open(\"cos_similarity.txt\",'r') as ip_file:\n",
    "    json_object=json.load(ip_file)\n",
    "col_sim = json_object[\"column_similarity\"]\n",
    "cat_sim = json_object[\"category_similarity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all functions needed to generate ontologies\n",
    "def get_synonyms(word):\n",
    "    meanings=set()\n",
    "    for synset in wordnet.synsets(word,pos=wordnet.NOUN):\n",
    "        for lemma in synset.lemmas():\n",
    "            meanings.add(lemma.name())\n",
    "    for synset in wordnet.synsets(word,pos=wordnet.NOUN):\n",
    "        for hypernym in synset.hypernyms():\n",
    "            meanings.add(hypernym.lemma_names()[0])\n",
    "    meanings.add(word)\n",
    "    return list(meanings)\n",
    "\n",
    "# takes input list and returns ontology as dictionary with every word in list as the key\n",
    "def generate_list_ontology(list1):\n",
    "    ontology={}\n",
    "    for word in list1:\n",
    "        ontology[word]=get_synonyms(word)\n",
    "    return ontology\n",
    "\n",
    "# takes the input as a schema and returns ontology for every column in the schema\n",
    "def generate_schema_ontology(input_schema):\n",
    "    ontology={}\n",
    "    for col in input_schema:\n",
    "        ontology[col]=get_synonyms(col)\n",
    "    return ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate cos similarity between two lists\n",
    "def cos_sim(list1, list2):\n",
    "    terms = set(list1).union(list2)\n",
    "    intersect = set(list1) & set(list2)\n",
    "    others = (set(list1)-intersect).union(set(list2)-intersect)\n",
    "    product=0\n",
    "    for word in terms:\n",
    "        if word in intersect:\n",
    "            product+=1\n",
    "    l1mag = math.sqrt(len(list1))\n",
    "    l2mag = math.sqrt(len(list2))\n",
    "    if len(list1)==0 or len(list2)==0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return product / (l1mag * l2mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both the following functions\n",
    "# col_only_list() and cat_and_col_list()\n",
    "# functions return all the posssible tables that are suitable matches with input schema\n",
    "\n",
    "# if input has only schema(columns and their dataypes)\n",
    "# it returns a list of all tables with one or more columns that match with input schema(or its ontology)\n",
    "def col_only_list(input_schema,input_sch_onto):\n",
    "    possible_tables={}\n",
    "    for file in all_schemas:\n",
    "        schema=all_schemas[file]\n",
    "        for col,d_type in schema.items():\n",
    "            if (col in input_schema) and (input_schema[col]==d_type):\n",
    "                if file in possible_tables:\n",
    "                    possible_tables[file].append(col)\n",
    "                else:\n",
    "                    possible_tables[file]=[]\n",
    "                    possible_tables[file].append(col)\n",
    "            else:\n",
    "                for a in input_sch_onto:\n",
    "                    if (col in input_sch_onto[a]) and (input_schema[a]==d_type):\n",
    "                        if file in possible_tables:\n",
    "                            possible_tables[file].append(a)\n",
    "                        else:\n",
    "                            possible_tables[file]=[]\n",
    "                            possible_tables[file].append(a)\n",
    "    return possible_tables\n",
    "\n",
    "# if input has categories as well as schema(columns and their dataypes)\n",
    "# we consider it a match under the assumption that at least 75% category match exists\n",
    "# then all tables that meet this criteria and have one or more columns that match are retured as a list \n",
    "def cat_and_col_list(input_categories,input_cat_onto,input_schema,input_sch_onto):\n",
    "    possible_tables={}\n",
    "    for file in all_categories:\n",
    "        cat_list=[]\n",
    "        category=all_categories[file]\n",
    "        for cat in category:\n",
    "            if cat in input_categories:\n",
    "                cat_list.append(cat)\n",
    "            else:\n",
    "                for cat1 in input_cat_onto:\n",
    "                    if cat in input_cat_onto[cat1]:\n",
    "                        cat_list.append(cat1)\n",
    "        cos_val=cos_sim(cat_list,input_categories)\n",
    "        if cos_val > 0.75 :\n",
    "            schema=all_schemas[file]\n",
    "            for col,d_type in schema.items():\n",
    "                if (col in input_schema) and (input_schema[col]==d_type):\n",
    "                    if file in possible_tables:\n",
    "                        possible_tables[file].append(col)\n",
    "                    else:\n",
    "                        possible_tables[file]=[]\n",
    "                        possible_tables[file].append(col)\n",
    "                else:\n",
    "                    for a in input_sch_onto:\n",
    "                        if (col in input_sch_onto[a]) and (input_schema[a]==d_type):\n",
    "                            if file in possible_tables:\n",
    "                                possible_tables[file].append(a)\n",
    "                            else:\n",
    "                                possible_tables[file]=[]\n",
    "                                possible_tables[file].append(a)\n",
    "    return possible_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function generates all possible combinations of list l taking elements n to 2 at a time and returns a dictionary\n",
    "def generate_all_combinations(l):\n",
    "    x={}\n",
    "    a=len(l)\n",
    "    for i in range(a,1,-1):\n",
    "        x[i]=list(combinations(l,i))\n",
    "    return x\n",
    "\n",
    "# function generates all matching columns between the res_cols schema and columns of table in fname\n",
    "def generate_matching_columns(res_cols,fname):\n",
    "    a=all_schemas[fname]\n",
    "    matching_columns={}\n",
    "    res_onto=generate_schema_ontology(res_cols)\n",
    "    for col , d_type in a.items():\n",
    "        if (col in res_cols) and (d_type==res_cols[col]):\n",
    "            matching_columns[col] = col\n",
    "        else:\n",
    "            for col_res in res_onto:\n",
    "                if (col in res_onto[col_res]) and (d_type==res_cols[col_res]):\n",
    "                    matching_columns[col] = col_res\n",
    "                    break\n",
    "    return matching_columns\n",
    "\n",
    "# returns a merged table of all tables given in input list l\n",
    "def merge_list(l):\n",
    "    t1=pd.read_csv(l[0])\n",
    "    t2=pd.read_csv(l[1])\n",
    "    a=all_schemas[l[0]]\n",
    "    b=all_schemas[l[1]]\n",
    "    matching_columns={}\n",
    "    a_onto=generate_schema_ontology(a)\n",
    "    for col , d_type in b.items():\n",
    "        if (col in a) and (d_type==a[col]):\n",
    "            matching_columns[col] = col\n",
    "        else:\n",
    "            for col_a in a_onto:\n",
    "                if (col in a_onto[col_a]) and (d_type==a[col_a]):\n",
    "                    matching_columns[col] = col_a\n",
    "                    break\n",
    "    t2.rename(columns = matching_columns,inplace=True)\n",
    "    res=t1.merge(t2,how='outer')\n",
    "    for fname in l[2:]:\n",
    "        d_types=[]\n",
    "        res_cols={}\n",
    "        for i in res.dtypes:\n",
    "            d_types.append(str(i))\n",
    "        for i,j in zip(res.columns,d_types):\n",
    "            res_cols[i]=j\n",
    "        matching_columns=generate_matching_columns(res_cols,fname)\n",
    "        t=pd.read_csv(fname)\n",
    "        t.rename(columns = matching_columns,inplace=True)\n",
    "        res=res.merge(t,how='outer')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics calculated :\n",
    "# 1)nan_score(number of nulls in each column)\n",
    "# 2)coverage_score(no of matching columns with input schema/total number of columns in input schema)\n",
    "# 3)completeness_score(a combination of coverage and nan scores to determine how complete the result dataset is)\n",
    "\n",
    "# nan score = {x : (no on nans in column/no of entries in column)} where x is each column in the table\n",
    "# gives the nan score(no on nans/no of entries in table) for each column in the input table\n",
    "def nan_score(table=-1,fname=-1):\n",
    "    if fname!=-1:\n",
    "        table=pd.read_csv(fname)\n",
    "    nan_count={}\n",
    "    a=len(table)\n",
    "    for i in table.columns:\n",
    "        x=a-table[i].count()\n",
    "        s=str(x)+'/'+str(a)\n",
    "        nan_count[i]=s\n",
    "    return nan_count\n",
    "\n",
    "#returns the coverage score and completeness score of a given table\n",
    "#coverage score is calculated as : \n",
    "# coverage = (no of columns matching with input schema/total number of columns in input schema)\n",
    "#completeness score is calculated as : \n",
    "# completeness = (sum(x*(non null entries)/(total entries in the column))/total number of columns in input schema) \n",
    "#  where x=1 if column present in input schema and x=0 if column is not present in the input schema\n",
    "def coverage_and_completeness(table):\n",
    "    ctr=0\n",
    "    comp=0\n",
    "    cols=table.columns\n",
    "    l=len(table)\n",
    "    for col in cols:\n",
    "        if col in input_schema:\n",
    "            ctr+=1\n",
    "            comp+=1*(1-sum(pd.isnull(table[col]))/l)\n",
    "    comp=comp/len(input_schema)\n",
    "    cov=ctr/len(input_schema)\n",
    "    return (cov,comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranking_display(comp_score,no_of_rows):\n",
    "    print()\n",
    "    comp_rank=sorted(comp_score,reverse=True)\n",
    "    count=0\n",
    "    l=sorted(no_of_rows, key=lambda k: no_of_rows[k],reverse=True)\n",
    "    for i in comp_rank:\n",
    "        if len(comp_score[i])==1:\n",
    "            count+=1\n",
    "            print(\"Rank \"+str(count)+\" : \",comp_score[i][0],\"\\t\\tcompleteness score : \",i,\"\\t\\tnumber of rows: \",no_of_rows[comp_score[i][0]])\n",
    "        else:\n",
    "            for j in l:\n",
    "                if j in comp_score[i]:\n",
    "                    count+=1\n",
    "                    print(\"Rank \"+str(count)+\" : \",j,\"\\t\\tcompleteness score : \",i,\"\\t\\tnumber of rows: \",no_of_rows[j])\n",
    "    print(\"\\nRanking Complete!!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a list of transformation functions\n",
    "tf_onto={}\n",
    "transform_funct_list={\"int64\":['average','sum','maximum','minimum','range','median','variance','standard deviation','mode','frequency','avg'],\"float64\":['average','sum','maximum','minimum','range','median','variance','standard deviation','mode','frequency','avg'],\"object\":[\"funct1\",\"funct2\",\"funct3\"]}\n",
    "for dtype,funct_list in transform_funct_list.items():\n",
    "    tf_onto[dtype]=generate_list_ontology(transform_funct_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes output tables schema(columns and data_types) as the input, compare it with input schema and transformations required and\n",
    "# returns a list of all the transformations applicable\n",
    "def get_possible_transformations(cols):\n",
    "    l={}\n",
    "    for col,tran in transformations.items():\n",
    "        if (col in cols):\n",
    "            for i in tran:\n",
    "                if i in transform_funct_list[input_schema[col]]:\n",
    "                    if col in l:\n",
    "                        l[col].append(i)\n",
    "                    else:\n",
    "                        l[col]=[]\n",
    "                        l[col].append(i)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function to print the individual tables names, their nan score, columns that match with input schema, \n",
    "# coverage score, and completeness score along with possible transformations if any\n",
    "def display_individual_matches(matching_tables,matching_tables_dict):\n",
    "    global comp_score\n",
    "    global no_of_rows\n",
    "    f=open(\"demo/results.txt\",'a')\n",
    "    if transformations==-1:\n",
    "        flag=0\n",
    "    else:\n",
    "        flag=1\n",
    "    for i in matching_tables:\n",
    "        print(i+\"(\"+all_tablenames[i]+\")\",file=f)\n",
    "        res=pd.read_csv(i)\n",
    "        cov,comp=coverage_and_completeness(res)\n",
    "        if flag==1:\n",
    "            cols=res.columns\n",
    "            l=get_possible_transformations(cols)\n",
    "            print(\"possible transformations are : \",l,file=f)\n",
    "            cov,comp=coverage_and_completeness(res)\n",
    "        print('Missing Values(NANs score): ',nan_score(fname=i),file=f)\n",
    "        print(\"Columns that match with input_schema:\\n \"+i+' : ',matching_tables_dict[i],file=f)\n",
    "        print(\"Coverage Score : \",cov,\"\\t Completeness Score : \",comp,\"\\t Number of Rows : \",len(res),file=f)\n",
    "        no_of_rows[i]=len(res)\n",
    "        if comp in comp_score:\n",
    "            comp_score[comp].append(i)\n",
    "        else:\n",
    "            comp_score[comp]=[]\n",
    "            comp_score[comp].append(i)\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this functions calls the col_only_list() or cat_and_col_list() based on input query requirements\n",
    "# it recieves a list of tbales with one or more columns matching with input schema and if categories involved then 75% category cos similarity with input schema\n",
    "# then we check for 75% column cosine cimilarity score and return a list of all tables having 75% column cosine similarity score\n",
    "def check_possible_matches():\n",
    "    input_sch_onto=generate_schema_ontology(input_schema)\n",
    "    if input_categories==-1:\n",
    "        print('only schema')\n",
    "        possible_tables=col_only_list(input_schema,input_sch_onto)\n",
    "    else:\n",
    "        print('category and schema\\n')\n",
    "        input_cat_onto=generate_list_ontology(input_categories)\n",
    "        possible_tables=cat_and_col_list(input_categories,input_cat_onto,input_schema,input_sch_onto)\n",
    "    matching_tables={}\n",
    "    for i in possible_tables:\n",
    "        cos_val=cos_sim(possible_tables[i],list(input_schema))\n",
    "        if cos_val>0.75:\n",
    "            matching_tables[i]=possible_tables[i]\n",
    "    return matching_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the main function that is to be invoked and will call all the required functions to obtain the required matches and merges\n",
    "# we have a list with all possible tables that are matches\n",
    "# we generate all combinations of them in order to merge them\n",
    "# a valid combination is one where every pair of tables have 50% cosine column and category similarity\n",
    "# then we merge them and calculate the coverage score, null score and the completeness score\n",
    "# all these details are displayed for each of the valid merges as well as individual tables\n",
    "# the output is displayed in the output file \"results.txt\" along with the outputs in csv form\n",
    "def get_matches():\n",
    "    global comp_score\n",
    "    global no_of_rows\n",
    "    global ctr\n",
    "    comp_score={}\n",
    "    no_of_rows={}\n",
    "    matching_tables_dict=check_possible_matches()\n",
    "    matching_tables=list(matching_tables_dict)\n",
    "    with open(\"demo/results.txt\",'a') as f:\n",
    "        print('******************************',file=f)\n",
    "        print(file=f)\n",
    "        print(\"All Possible Matches\",file=f)\n",
    "        print(file=f)\n",
    "        print(\"Matching Tables : \",matching_tables,file=f)\n",
    "        for i in matching_tables:\n",
    "            print(i,all_tablenames[i])\n",
    "        if transformations==-1:\n",
    "            print(\"\\nNo Transformations In Input Schema\",file=f)\n",
    "            print(file=f)\n",
    "        else:\n",
    "            print('\\nTransformations detected from input are : ',transformations,file=f)\n",
    "            print(file=f)\n",
    "    op_str1='demo/'\n",
    "    op_str2='.csv'\n",
    "    x=len(matching_tables)\n",
    "    print(matching_tables)\n",
    "    if x==0:\n",
    "        with open(\"demo/results.txt\",'a') as f:\n",
    "            print(\"NO MATCHES FOUND\",file=f)\n",
    "            print(file=f)\n",
    "    elif x==1:\n",
    "        display_individual_matches(matching_tables,matching_tables_dict)\n",
    "    elif x==2:\n",
    "        a=matching_tables[0]+' : '+matching_tables[1]\n",
    "        b=matching_tables[1]+' : '+matching_tables[0]\n",
    "        if (a in cat_sim) or (b in cat_sim):\n",
    "            if (cat_sim[a]>.50 and col_sim[a]>0.50) or (cat_sim[b]>.50 and col_sim[b]>0.50):\n",
    "                ctr+=1\n",
    "                res=res=merge_list([matching_tables[0],matching_tables[1]])\n",
    "                op_string=op_str1+str(ctr)+op_str2\n",
    "                res.to_csv(op_string,sep=',', index=False)\n",
    "                cols=res.columns\n",
    "                cov,comp=coverage_and_completeness(res)\n",
    "                with open(\"demo/results.txt\",'a') as f:\n",
    "                    print(op_string,file=f)\n",
    "                    if transformations!=-1:\n",
    "                        l=get_possible_transformations(cols)\n",
    "                        print(\"possible transformations are : \",l,file=f)\n",
    "                    print('Missing Values(NANs): ',nan_score(table=res),file=f)\n",
    "                    print(\"Columns that match with input_schema: \",file=f)\n",
    "                    for j in matching_tables:\n",
    "                        print(j+' : ',matching_tables_dict[j],file=f)\n",
    "                    print(\"Coverage Score : \",cov,\"\\t Completeness Score : \",comp,\"\\t Number of Rows : \",len(res),file=f)\n",
    "                    no_of_rows[str(ctr)+op_str2]=len(res)\n",
    "                    if comp in comp_score:\n",
    "                        comp_score[comp].append(str(ctr)+op_str2)\n",
    "                    else:\n",
    "                        comp_score[comp]=[]\n",
    "                        comp_score[comp].append(str(ctr)+op_str2)\n",
    "                    print(file=f)\n",
    "        display_individual_matches(matching_tables,matching_tables_dict)\n",
    "    else:\n",
    "        count_comb=len(matching_tables)\n",
    "        all_combos=generate_all_combinations(matching_tables)\n",
    "        for i in range(count_comb,1,-1):\n",
    "            for l in all_combos[i]:\n",
    "                a=list(l)\n",
    "                comb=list(combinations(a,2))\n",
    "                flag=0\n",
    "                for pair in comb:\n",
    "                    if flag==0:\n",
    "                        t1,t2=pair\n",
    "                        if ((t1+' : '+t2) in cat_sim):\n",
    "                            if (cat_sim[t1+' : '+t2]>=.50 and col_sim[t1+' : '+t2]>=0.50):\n",
    "                                pass\n",
    "                            else:\n",
    "                                flag=1\n",
    "                        else:\n",
    "                            flag=1\n",
    "                    else:\n",
    "                        break\n",
    "                if flag==0:\n",
    "                    res=merge_list(a)\n",
    "                    ctr+=1\n",
    "                    op_string=op_str1+str(ctr)+op_str2\n",
    "                    res.to_csv(op_string,sep=',', index=False)\n",
    "                    cols=res.columns\n",
    "                    cov,comp=coverage_and_completeness(res)\n",
    "                    with open(\"demo/results.txt\",'a') as f:\n",
    "                        print(str(ctr)+op_str2+' : ',end='',file=f)\n",
    "                        for j in a:\n",
    "                            print(j+\"(\"+all_tablenames[j]+\")\"+'\\t\\t',end='',file=f)\n",
    "                        print(file=f)\n",
    "                        if transformations!=-1:\n",
    "                            l=get_possible_transformations(cols)\n",
    "                            print(\"possible transformations are : \",l,file=f)\n",
    "                        print('Missing Values(NANs): ',nan_score(table=res),file=f)\n",
    "                        print(\"Columns that match with input_schema: \",file=f)\n",
    "                        for j in a:\n",
    "                            print(j+' : ',matching_tables_dict[j],file=f)\n",
    "                        print(\"Coverage Score : \",cov,\"\\t Completeness Score : \",comp,\"\\t Number of Rows : \",len(res),file=f)\n",
    "                        no_of_rows[str(ctr)+op_str2]=len(res)\n",
    "                        if comp in comp_score:\n",
    "                            comp_score[comp].append(str(ctr)+op_str2)\n",
    "                        else:\n",
    "                            comp_score[comp]=[]\n",
    "                            comp_score[comp].append(str(ctr)+op_str2)\n",
    "                        print(file=f)\n",
    "        display_individual_matches(matching_tables,matching_tables_dict)\n",
    "    with open(\"demo/results.txt\",'a') as f:\n",
    "        print('******************************',file=f)\n",
    "    ranking_display(comp_score,no_of_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT 1\n",
      "{\"schema\": {\"Round\": \"int64\", \"Round.1\": \"object\", \"Circuit\": \"object\", \"Date\": \"object\", \"Pole Position\": \"object\", \"Fastest Lap\": \"object\", \"Winning Driver\": \"object\"}}\n",
      "\n",
      "only schema\n",
      "203_408.csv 1989 Formula One season\n",
      "203_514.csv 2008 Superleague Formula season\n",
      "204_253.csv 1990 Superbike World Championship season\n",
      "204_40.csv 2008 Superbike World Championship season\n",
      "204_455.csv 1989 Formula One season\n",
      "204_569.csv 1998 Swedish Touring Car Championship season\n",
      "204_63.csv 2002 Italian Formula Three season\n",
      "['203_408.csv', '203_514.csv', '204_253.csv', '204_40.csv', '204_455.csv', '204_569.csv', '204_63.csv']\n",
      "\n",
      "Rank 1 :  204_569.csv \t\tcompleteness score :  0.9285714285714286 \t\tnumber of rows:  12\n",
      "Rank 2 :  8.csv \t\tcompleteness score :  0.8796992481203008 \t\tnumber of rows:  38\n",
      "Rank 3 :  10.csv \t\tcompleteness score :  0.8785714285714284 \t\tnumber of rows:  40\n",
      "Rank 4 :  2.csv \t\tcompleteness score :  0.8701298701298701 \t\tnumber of rows:  66\n",
      "Rank 5 :  7.csv \t\tcompleteness score :  0.8571428571428571 \t\tnumber of rows:  54\n",
      "Rank 6 :  204_40.csv \t\tcompleteness score :  0.8571428571428571 \t\tnumber of rows:  28\n",
      "Rank 7 :  204_253.csv \t\tcompleteness score :  0.8571428571428571 \t\tnumber of rows:  26\n",
      "Rank 8 :  1.csv \t\tcompleteness score :  0.8241758241758241 \t\tnumber of rows:  52\n",
      "Rank 9 :  3.csv \t\tcompleteness score :  0.8206686930091186 \t\tnumber of rows:  47\n",
      "Rank 10 :  5.csv \t\tcompleteness score :  0.7928571428571428 \t\tnumber of rows:  40\n",
      "Rank 11 :  6.csv \t\tcompleteness score :  0.7857142857142857 \t\tnumber of rows:  24\n",
      "Rank 12 :  9.csv \t\tcompleteness score :  0.7836734693877551 \t\tnumber of rows:  35\n",
      "Rank 13 :  11.csv \t\tcompleteness score :  0.7755102040816325 \t\tnumber of rows:  21\n",
      "Rank 14 :  203_514.csv \t\tcompleteness score :  0.6428571428571429 \t\tnumber of rows:  12\n",
      "Rank 15 :  4.csv \t\tcompleteness score :  0.5714285714285714 \t\tnumber of rows:  16\n",
      "Rank 16 :  203_408.csv \t\tcompleteness score :  0.5714285714285714 \t\tnumber of rows:  16\n",
      "Rank 17 :  204_455.csv \t\tcompleteness score :  0.5714285714285714 \t\tnumber of rows:  16\n",
      "Rank 18 :  204_63.csv \t\tcompleteness score :  0.5714285714285714 \t\tnumber of rows:  9\n",
      "\n",
      "Ranking Complete!!\n",
      "\n",
      "INPUT 2\n",
      "{\"schema\": {\"Round\": \"int64\", \"Round.1\": \"object\", \"Circuit\": \"object\", \"Date\": \"object\", \"Pole Position\": \"object\", \"Fastest Lap\": \"object\", \"Winning Driver\": \"object\"},\"categories\": [\"motorsport\", \"car\", \"seasons\"]}\n",
      "\n",
      "category and schema\n",
      "\n",
      "203_514.csv 2008 Superleague Formula season\n",
      "204_253.csv 1990 Superbike World Championship season\n",
      "204_569.csv 1998 Swedish Touring Car Championship season\n",
      "204_63.csv 2002 Italian Formula Three season\n",
      "['203_514.csv', '204_253.csv', '204_569.csv', '204_63.csv']\n",
      "\n",
      "Rank 1 :  204_569.csv \t\tcompleteness score :  0.9285714285714286 \t\tnumber of rows:  12\n",
      "Rank 2 :  14.csv \t\tcompleteness score :  0.8796992481203008 \t\tnumber of rows:  38\n",
      "Rank 3 :  204_253.csv \t\tcompleteness score :  0.8571428571428571 \t\tnumber of rows:  26\n",
      "Rank 4 :  12.csv \t\tcompleteness score :  0.8206686930091186 \t\tnumber of rows:  47\n",
      "Rank 5 :  13.csv \t\tcompleteness score :  0.7857142857142857 \t\tnumber of rows:  24\n",
      "Rank 6 :  15.csv \t\tcompleteness score :  0.7836734693877551 \t\tnumber of rows:  35\n",
      "Rank 7 :  16.csv \t\tcompleteness score :  0.7755102040816325 \t\tnumber of rows:  21\n",
      "Rank 8 :  203_514.csv \t\tcompleteness score :  0.6428571428571429 \t\tnumber of rows:  12\n",
      "Rank 9 :  204_63.csv \t\tcompleteness score :  0.5714285714285714 \t\tnumber of rows:  9\n",
      "\n",
      "Ranking Complete!!\n",
      "\n",
      "INPUT 3\n",
      "{\"schema\": {\"Date\": \"object\", \"Nationality\": \"object\", \"Tonnage (GRT)\": \"int64\", \"Fate\": \"object\"},\"transformations\":{\"Tonnage (GRT)\":[\"sum\",\"funct1\"],\"Nationality\":[\"funct1\"]}}\n",
      "\n",
      "only schema\n",
      "202_117.csv German submarine U-559\n",
      "203_148.csv German submarine U-9 (1935)\n",
      "203_268.csv German submarine U-502\n",
      "204_100.csv Hans-Rudolf Rosing\n",
      "['202_117.csv', '203_148.csv', '203_268.csv', '204_100.csv']\n",
      "\n",
      "Rank 1 :  20.csv \t\tcompleteness score :  1.0 \t\tnumber of rows:  25\n",
      "Rank 2 :  203_268.csv \t\tcompleteness score :  1.0 \t\tnumber of rows:  16\n",
      "Rank 3 :  203_148.csv \t\tcompleteness score :  1.0 \t\tnumber of rows:  9\n",
      "Rank 4 :  17.csv \t\tcompleteness score :  0.9516129032258065 \t\tnumber of rows:  31\n",
      "Rank 5 :  19.csv \t\tcompleteness score :  0.9318181818181819 \t\tnumber of rows:  22\n",
      "Rank 6 :  18.csv \t\tcompleteness score :  0.9 \t\tnumber of rows:  15\n",
      "Rank 7 :  204_100.csv \t\tcompleteness score :  0.75 \t\tnumber of rows:  13\n",
      "Rank 8 :  202_117.csv \t\tcompleteness score :  0.75 \t\tnumber of rows:  6\n",
      "\n",
      "Ranking Complete!!\n",
      "\n",
      "INPUT 4\n",
      "{\"schema\": {\"Date\": \"object\", \"Nationality\": \"object\", \"Tonnage (GRT)\": \"int64\", \"Fate\": \"object\"},\"transformations\":{\"Tonnage (GRT)\":[\"sum\"],\"Nationality\":[\"funct1\"]},\"categories\":[\"submarines\", \"war\", \"ships\", \"sea\", \"aircraft\", \"germany\", \"shipwrecks\", \"maritime\", \"world\", \"german\", \"sunk\", \"navy\", \"uboats\"]}\n",
      "category and schema\n",
      "\n",
      "202_117.csv German submarine U-559\n",
      "203_148.csv German submarine U-9 (1935)\n",
      "203_268.csv German submarine U-502\n",
      "['202_117.csv', '203_148.csv', '203_268.csv']\n",
      "\n",
      "Rank 1 :  24.csv \t\tcompleteness score :  1.0 \t\tnumber of rows:  25\n",
      "Rank 2 :  203_268.csv \t\tcompleteness score :  1.0 \t\tnumber of rows:  16\n",
      "Rank 3 :  203_148.csv \t\tcompleteness score :  1.0 \t\tnumber of rows:  9\n",
      "Rank 4 :  21.csv \t\tcompleteness score :  0.9516129032258065 \t\tnumber of rows:  31\n",
      "Rank 5 :  23.csv \t\tcompleteness score :  0.9318181818181819 \t\tnumber of rows:  22\n",
      "Rank 6 :  22.csv \t\tcompleteness score :  0.9 \t\tnumber of rows:  15\n",
      "Rank 7 :  202_117.csv \t\tcompleteness score :  0.75 \t\tnumber of rows:  6\n",
      "\n",
      "Ranking Complete!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ctr=0\n",
    "comp_score={}\n",
    "no_of_rows={}\n",
    "with open(\"demo_inputs.txt\") as ip_file:\n",
    "    count=0\n",
    "    for line in ip_file.readlines():\n",
    "        ip_schema=json.loads(line)\n",
    "        count+=1\n",
    "        print(\"INPUT \"+str(count))\n",
    "        print(line)\n",
    "        input_schema=ip_schema[\"schema\"]\n",
    "        if \"categories\" in ip_schema:\n",
    "            input_categories=ip_schema[\"categories\"]\n",
    "        else:\n",
    "            input_categories=-1\n",
    "        if \"transformations\" in ip_schema:\n",
    "            transformations=ip_schema[\"transformations\"]\n",
    "        else:\n",
    "            transformations=-1\n",
    "        with open(\"demo/results.txt\",'a') as f:\n",
    "            print(\"-------------------------------\",file=f)\n",
    "            print(\"INPUT \"+str(count),file=f)\n",
    "            print(\"Input Schema : \",file=f)\n",
    "            print(line,file=f)\n",
    "        get_matches()\n",
    "        with open(\"demo/results.txt\",'a') as f:\n",
    "            print(\"-------------------------------\",file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
